{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digits_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidabelin/capstone_project_udacity_mle/blob/main/digits_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHK6DyunSbs4"
      },
      "source": [
        "# Handwritten Arithmetic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLy3pthUS0D2"
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "#%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "#import matplotlib.image as Image\n",
        "#import seaborn as sbn\n",
        " \n",
        "#import zipfile\n",
        "#import math\n",
        "import random as rd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import os, signal\n",
        "from IPython.display import clear_output\n",
        " \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "#from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, InputLayer, Input\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8Y3AFF5pgJZ"
      },
      "source": [
        "#@ title VARIABLES\n",
        "### Load raw MNIST data as tuples of numpy arrays\n",
        "### (example images, labels)\n",
        "traintuple, testuple = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "### Number of images to generate for each category\n",
        "N_val = 5000        # used while training\n",
        "N_test = 10000      # used for evaluation\n",
        "N_train = 100000    # used for training\n",
        "\n",
        "### Training hyperparameters, same for all models\n",
        "batch_size = 500\n",
        "validation_batch_size = 100\n",
        "epochs = 5\n",
        "learning_rate = 0.005\n",
        "input_layer = layers.Input(shape=(28, 56, 1))\n",
        "\n",
        "image_size = (28,56) # shape of target image\n",
        "model_kinds = [\"miniNN\", \"CNN\", \"DNN\", \"comboNN\", \"ocNN\"]\n",
        "noise_levels = [\"no\", \"low\", \"high\", \"var\"]\n",
        "training_keys = []\n",
        "for nc in noise_levels:\n",
        "    for k in model_kinds:\n",
        "        training_keys.append((k, nc))\n",
        "### Translate noise level to description\n",
        "noise2condition = {nc:d for (nc, d) in zip(noise_levels,\n",
        "                                        [\"No noise\", \"Low noise\", \"High noise\", \"Variable noise\"])} "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY6KJV6z6l7_"
      },
      "source": [
        "# Construct the Input Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X6jzIZm1vDZ"
      },
      "source": [
        "#@ title Define Functions\n",
        "def scale_array(arr):\n",
        "    '''Scale an array of numbers.\n",
        "        Params: numpy array of values to be scaled\n",
        "        Returns: the same array with values scaled to (0,1)\n",
        "    '''\n",
        "    max = np.max(arr)\n",
        "    min = np.min(arr)\n",
        "    scaled = (arr - min)/(max - min)\n",
        "    return scaled\n",
        "print(\"Loaded function scale_array(arr)\")\n",
        "\n",
        "def get_noise(image, noise_condition=\"var\"):\n",
        "    ''' Add a normal disitribution of noise to an image\n",
        "        Params: image: numpy array with values (0,1)\n",
        "                noise_factor: one of \"no\", \"low\", \"high\", \"var\"\n",
        "        Returns: noisy image array re-normalized to (0,1)\n",
        "    '''\n",
        "    cond2num = {\"no\":0, \n",
        "                \"low\":0.3, \n",
        "                \"high\":1.3, \n",
        "                \"var\":rd.random()} # Different value each call\n",
        "    noise_factor = np.std(image)*cond2num[noise_condition]\n",
        "    noise = np.random.normal(np.mean(image), \n",
        "                             noise_factor, \n",
        "                             size=image.shape)\n",
        "    ## TODO: Unfortunately have to call scale_array() for second time\n",
        "    # Renormalize combined image to (0,1)\n",
        "    return scale_array(noise + image)\n",
        "print(\"Loaded function get_noise(image, noise_condition)\")\n",
        "\n",
        "def doubleDigits(datatuple, nc=\"no\"):\n",
        "    ''' Merge two single-digit images into one double-digit image.\n",
        "        Params: images: np.array with shape (N,28,28), values 0 to 255\n",
        "                answers: np.array with shape (N,), values 0 to 9\n",
        "                nf: \"noise factor\" (float) is multiples/fractions of image std\n",
        "        Returns: noisy double-digit image as a numpy array with shape (28,56),\n",
        "                with values normed to (0.0, 1.0); \n",
        "                and the corresponding label, with values now 0 to 99\n",
        "    '''\n",
        "    (images,answers)=datatuple  # or x, y\n",
        "    \n",
        "    # Randomly select left and right single digit images\n",
        "    # with values 0 to 9, from the same raw training or test\n",
        "    # data set\n",
        "    left_index = rd.randrange(0, len(answers))   \n",
        "    right_index = rd.randrange(0, len(answers))\n",
        "    # Calculate double digit label 0 to 99\n",
        "    answer = answers[left_index]*10 + answers[right_index]\n",
        "    \n",
        "    # Have to scale them here because they have\n",
        "    # different distributions of pixel values\n",
        "    left_scaled = scale_array(images[left_index])\n",
        "    right_scaled = scale_array(images[right_index])\n",
        "\n",
        "    # Make background array with shape (28,56)\n",
        "    image = np.zeros(image_size)\n",
        "    # Group digits closer to middle of new image\n",
        "    width = image_size[1]\n",
        "    half_width = width//2  \n",
        "    # Shift left digit to the right\n",
        "    image[:,8:half_width+4] = left_scaled[:,4:half_width]\n",
        "    # Shift right digit to the left\n",
        "    image[:,half_width-4:width-8] += right_scaled[:,0:half_width-4]\n",
        "    # In case some bright pixels overlap\n",
        "    image = image.clip(0,1)\n",
        "    # Add noise to the new double digit image\n",
        "    ####### TODO: this will scale it again for second time to [0,1] ###\n",
        "    # Call get_noise() to overlay a Normal distribution\n",
        "    # of random pixel values centered on the mean of pixel values\n",
        "    # in the image and with \"noise_factor\" as the width of the\n",
        "    # distribution, from 0 (no distortion) to +inf (uniform distribution),\n",
        "    # in multiples or fractions of the standard deviation of the\n",
        "    # pixel values in image, and rescale pixel values to (0,1)\n",
        "    image = get_noise(image, nc)  \n",
        "    return image, int(answer)              \n",
        "print(\"Loaded function doubleDigits(datatuple, nc=no)\")\n",
        "\n",
        "def getDoubleDigits(datatuple, how_many=1, nc=\"no\"):\n",
        "    ''' Aggregate a given number of two-digit images, with or without noise\n",
        "        Params: image array of size (N, (image size)),\n",
        "                answers array of size (N,)\n",
        "        Returns: a single 28x56 double-digit image and\n",
        "                 the corresponding array of int labels\n",
        "    '''\n",
        "    yy = np.zeros((how_many,),dtype=int)\n",
        "    xx = np.zeros((how_many, image_size[0], image_size[1]))\n",
        "    for i in range(how_many):\n",
        "        dd, ans = doubleDigits(datatuple, nc)\n",
        "        yy[i] = ans\n",
        "        xx[i] = dd\n",
        "    return (xx, yy)\n",
        "print(\"Loaded function getDoubleDigits(datatuple,how_many=1,nc=no)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWlp5N49ByVb"
      },
      "source": [
        "def plotLearningCurves(history):\n",
        "    acc = history['acc']\n",
        "    val_acc = history['val_acc']\n",
        "    epochs = range(len(acc))\n",
        "    plt.plot(epochs, acc)\n",
        "    plt.plot(epochs, val_acc)\n",
        "    plt.title('Training and validation accuracy')\n",
        "print(\"Loaded function plotLearningCurves(history)\")\n",
        "\n",
        "def show_layers(model, output_model):\n",
        "    # Take random image from the training set.\n",
        "    rindex = rd.randrange(y_test.shape[0])\n",
        "    ans = y_test[rindex]\n",
        "    img = x_test[rindex]\n",
        "    img = img.reshape((1,) + img.shape)    # np shape (1, 28, 28, 1)\n",
        "    gue = model.predict(img)\n",
        "    print(\"Answer:\",ans, \"\\tGuess:\",np.argmax(gue))\n",
        "    plt.figure(figsize=(3,3))\n",
        "    plt.imshow(img[0,:,:,0], cmap=\"binary_r\")                 # np shape (28, 28)\n",
        "\n",
        "    layer_output_maps = output_model.predict( img )\n",
        "    layer_names = [layer.name for layer in model.layers[1:]]\n",
        "    for layer_name, layer_map in zip(layer_names, layer_output_maps):\n",
        "        if len(layer_map.shape) == 4:# and not 'max_pooling' in layer_name:\n",
        "            n_maps = layer_map.shape[-1]  # number of maps\n",
        "            if n_maps > 10:\n",
        "                n_maps = 10\n",
        "            # Map has shape (1, rows, columns, n_features)\n",
        "            rows = layer_map.shape[1]\n",
        "            cols = layer_map.shape[2]\n",
        "            image_grid = np.zeros((rows, cols * n_maps))\n",
        "            \n",
        "            for i in range(n_maps):\n",
        "                x = layer_map[0, :, :, i]\n",
        "                x *= 255.0\n",
        "                image_grid[:, i * cols : (i + 1) * cols] = x\n",
        "                image_grid[:,i*cols] = 255.\n",
        "                image_grid[:,i*cols+1] = 0.\n",
        "                \n",
        "            scale = 2.           \n",
        "            plt.figure(figsize=(scale * n_maps, scale))\n",
        "            plt.title(layer_name)\n",
        "            plt.grid(False)\n",
        "            plt.imshow(image_grid, cmap='gray')\n",
        "print(\"Loaded function show_layers(model, output_model)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJg3Wg6FBXLu"
      },
      "source": [
        "def guessing(model,n=1,return_image=False):\n",
        "    answers, guesses, pA, pG = [],[],[],[]\n",
        "    for count in range(n):\n",
        "        rindex = rd.randrange(y_test.shape[0])\n",
        "        ans = y_test[rindex]\n",
        "        img = x_test[rindex]\n",
        "        img = img.reshape((1,) + img.shape)    # eg. 1x50x50x1\n",
        "        guess_set = model.predict(img).flatten()\n",
        "        guess = np.argmax(guess_set)\n",
        "        answers += [ans]\n",
        "        guesses += [guess]\n",
        "        pG += [guess_set[guess]]\n",
        "        pA += [guess_set[ans]]\n",
        "\n",
        "        print(\"Answer\",ans,\"\\tGuess\",guess, \"\\tp(A)\",round(pA[count],2),\"\\tp(G)\",round(pG[count],2))\n",
        "        if count%10 == 0:\n",
        "            print ('Processing...',count,\"...\")\n",
        "            clear_output(wait=True)\n",
        "\n",
        "    if return_image:\n",
        "        return answers, guesses, pA, pG, img\n",
        "    else:\n",
        "        return answers, guesses, pA, pG\n",
        "print(\"Loaded function guessing(model,n=1,return_image=False)\")\n",
        "\n",
        "def get_guesses(m,n=1):\n",
        "    results = pd.DataFrame(columns=['Answer','Guess','P(A)','P(G)'])\n",
        "    results['Answer'],\n",
        "    results['Guess'],\n",
        "    results['P(A)'],\n",
        "    results['P(G)'] = guessing(n, m) \n",
        "    return results\n",
        "print(\"Loaded function get_guesses(m,n=1)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiFcn-Bm4dnX"
      },
      "source": [
        "### Check a random sampling\n",
        "nc = rd.choice(noise_levels)\n",
        "x, y = getDoubleDigits(traintuple, nc=nc, how_many=9)\n",
        "print('Answers:',y,\"\\nNoise:\",nc) \n",
        "\n",
        "f,a = plt.subplots(3,3,True,True)\n",
        "f.set_size_inches(15,7)\n",
        "a=a.reshape(9,)\n",
        "for i in range(9):  # 0 to 7\n",
        "    #a = f.add_subplot(3,3,i+1) \n",
        "    a[i].imshow(x[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YQljE-wizDw"
      },
      "source": [
        "######### GENERATE DD TRAINING DATA FOR ALL NOISE CONDITIONS ###########\n",
        "train_data = {} # {noise level : tuple of training data (x,y)}\n",
        "val_data = {}   # {noise level : val data (x,y)}\n",
        "test_data = {}  # {noise level : test data (x,y)}\n",
        "\n",
        "for nc in noise_levels:\n",
        "    ####### Make a set of training, validation, and test images\n",
        "    ####### One set each for each noise level \n",
        "    print (\"Noise level:\", noise2condition[nc])\n",
        "    x_train, y_train = getDoubleDigits(traintuple, N_train, nc=nc)\n",
        "    print(\"Made\",N_train,\"new double-digit images to train on.\")\n",
        "    # Test and validation sets constructed from MNIST test images\n",
        "    x_val, y_val = getDoubleDigits(testuple, N_val, nc=nc)\n",
        "    print(\"Made\",N_val,\"new double-digit images to validate on.\")\n",
        "    x_test, y_test = getDoubleDigits(testuple, N_test, nc=nc)\n",
        "    print(\"Made\",N_test,\"new double-digit images to test on.\")\n",
        "\n",
        "    ######  Add a black-and-white channels dimension\n",
        "    x_train = x_train[..., np.newaxis].astype(\"float32\")\n",
        "    x_val = x_val[..., np.newaxis].astype(\"float32\")\n",
        "    x_test = x_test[..., np.newaxis].astype(\"float32\")\n",
        "\n",
        "    train_data.update({nc:(x_train, y_train)})\n",
        "    val_data.update({nc:(x_val, y_val)}) \n",
        "    test_data.update({nc:(x_test, y_test)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oqBkNBJmtUv"
      },
      "source": [
        "# Build models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1BOIdzZa302",
        "cellView": "form"
      },
      "source": [
        "#########################\n",
        "#@title ORIGINAL Build a model for each noise condition\n",
        "\n",
        "def original_buildModel(key):\n",
        "    '''Builds a model of each kind, for each noise level\n",
        "        params: a tuple of \"model kind\" and \"noise level\"\n",
        "        returns: a model of the specified kind,\n",
        "                 dedicated to noise of the specified level\n",
        "    '''\n",
        "    (model_kind, noise) = key\n",
        "    ## Same input for all layers\n",
        "    input_layer = layers.Input(shape=(image_size[0], image_size[1], 1))\n",
        "        \n",
        "    if \"miniNN\" in model_kind:\n",
        "        ########### Build miniNN as \"baseline\" model \n",
        "        x = layers.Flatten()(input_layer)\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        miniNN = Model(input_layer, output_layer, name=\"miniNN_\"+noise)\n",
        "        print (\"built\", miniNN.name)\n",
        "        return miniNN\n",
        "\n",
        "    elif \"CNN\" in model_kind:\n",
        "        ########### Build CNN\n",
        "        x = layers.Conv2D(20, 2, padding='same', activation='relu')(input_layer)\n",
        "        x = layers.AveragePooling2D(2)(x)\n",
        "        x = layers.Conv2D(30, 3, activation='relu')(x) \n",
        "        x = layers.MaxPooling2D(2)(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        CNN = Model(input_layer, output_layer, name=\"CNN_\"+noise)\n",
        "        print (\"built\", CNN.name)\n",
        "        return CNN\n",
        "    \n",
        "    elif \"DNN\" in model_kind:\n",
        "        ########### Build DNN\n",
        "        x = layers.Flatten()(input_layer)\n",
        "        x = layers.Dense(1000, activation='relu')(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        x = layers.Dense(500, activation='relu')(x) \n",
        "        x = layers.Dropout(0.1)(x)\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        DNN = Model(input_layer, output_layer, name=\"DNN_\"+noise)\n",
        "        print (\"built\", DNN.name)\n",
        "        return DNN\n",
        "\n",
        "    elif \"comboNN\" in model_kind:\n",
        "        ########### Build comboNN, hybrid CNN/DNN: \n",
        "        x = layers.Conv2D(20, 3, activation='relu')(input_layer)\n",
        "        x = layers.AveragePooling2D(2)(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dense(200, activation='relu')(x)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        comboNN = Model(input_layer, output_layer, name=\"comboNN_\"+noise)\n",
        "        print (\"built\", comboNN.name)\n",
        "        return comboNN\n",
        "\n",
        "    elif \"ocNN\" in model_kind:\n",
        "        ########### Build \"overcomplicated\" model ocNN ########### \n",
        "        x = layers.Flatten()(input_layer)\n",
        "        x = layers.Dense(1000, activation='relu')(x)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "\n",
        "        x1 = layers.Dense(600, activation='relu')(x) \n",
        "        x1 = layers.Dropout(0.3)(x1)\n",
        "\n",
        "        x2 = layers.Dense(200, activation='relu')(x) \n",
        "        x2 = layers.Dropout(0.1)(x2)\n",
        "\n",
        "        y = layers.Conv2D(20, 2, padding='same', activation='relu')(input_layer)\n",
        "        y = layers.MaxPooling2D(2)(y)\n",
        "\n",
        "        y1 = layers.Conv2D(20, 3, activation='relu')(y) \n",
        "        y1 = layers.AveragePooling2D(2)(y1)\n",
        "        y1 = layers.Flatten()(y1)\n",
        "        y1 = layers.Dropout(0.2)(y1)\n",
        "\n",
        "        y2 = layers.Conv2D(20, 2, activation='relu')(y) \n",
        "        y2 = layers.Conv2D(20, 2, activation='relu')(y2)\n",
        "        y2 = layers.MaxPooling2D(2)(y2)\n",
        "        y2 = layers.Flatten()(y2)\n",
        "        y2 = layers.Dropout(0.1)(y2)\n",
        "\n",
        "        z1 = layers.Concatenate()([x1,y1])\n",
        "        z1 = layers.Dropout(0.3)(z1)\n",
        "\n",
        "        z2 = layers.Concatenate()([x2,y2])\n",
        "        z2 = layers.Dropout(0.1)(z2)\n",
        "\n",
        "        z = layers.Concatenate()([z1,z2])\n",
        "        z = layers.Dense(1000,activation=\"relu\")(z)\n",
        "        z = layers.Dropout(0.3)(z)\n",
        "\n",
        "        logits_layer = layers.Dense(100, activation='softmax')(z) \n",
        "        ocNN = Model(input_layer, logits_layer, name=\"ocNN_\"+noise)\n",
        "        print (\"built\", ocNN.name)\n",
        "        return ocNN\n",
        "    \n",
        "    else:\n",
        "        print (\"Could not find that kind of model.\")\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtngjIXgUvws"
      },
      "source": [
        "def buildModel(key):\n",
        "    '''Builds a model of each kind, for each noise level\n",
        "        params: a tuple of \"model kind\" and \"noise level\"\n",
        "        returns: a model of the specified kind,\n",
        "                 dedicated to noise of the specified level\n",
        "    '''\n",
        "    (model_kind, noise) = key\n",
        "    ## Same input for all layers\n",
        "    input_layer = layers.Input(shape=(image_size[0], image_size[1], 1))\n",
        "        \n",
        "    if \"miniNN\" in model_kind:\n",
        "        ########### Build miniNN as \"baseline\" model \n",
        "        x = layers.Flatten()(input_layer)\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        miniNN = Model(input_layer, output_layer, name=\"miniNN_\"+noise)\n",
        "        print (\"built\", miniNN.name)\n",
        "        return miniNN\n",
        "\n",
        "    elif \"CNN\" in model_kind:\n",
        "        ########### Build CNN\n",
        "        x = layers.Conv2D(20, 3, activation='relu')(input_layer)#, padding='same'\n",
        "        x = layers.MaxPooling2D(2)(x)\n",
        "        x = layers.Conv2D(30, 2, activation='relu')(x) \n",
        "        x = layers.AveragePooling2D(2)(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        CNN = Model(input_layer, output_layer, name=\"CNN_\"+noise)\n",
        "        print (\"built\", CNN.name)\n",
        "        return CNN\n",
        "    \n",
        "    elif \"DNN\" in model_kind:\n",
        "        ########### Build DNN\n",
        "        x = layers.Flatten()(input_layer)\n",
        "        x1 = layers.Dense(400, activation='relu')(x)\n",
        "        x1 = layers.Dropout(0.2)(x1)\n",
        "\n",
        "        x2 = layers.Dense(400, activation='relu')(x) \n",
        "        x2 = layers.Dropout(0.2)(x2)\n",
        "\n",
        "        x3 = layers.Concatenate()([x1,x2])\n",
        "        x3 = layers.Dense(400, activation='relu')(x3)\n",
        "        x3 = layers.Dropout(0.2)(x3) \n",
        "\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x3)\n",
        "        DNN = Model(input_layer, output_layer, name=\"DNN_\"+noise)\n",
        "        print (\"built\", DNN.name)\n",
        "        return DNN\n",
        "\n",
        "    elif \"comboNN\" in model_kind:\n",
        "        ########### Build comboNN, hybrid CNN/DNN: \n",
        "        x = layers.Conv2D(20, 3, activation='relu')(input_layer)\n",
        "        x = layers.AveragePooling2D(2)(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dense(500, activation='relu')(x)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        comboNN = Model(input_layer, output_layer, name=\"comboNN_\"+noise)\n",
        "        print (\"built\", comboNN.name)\n",
        "        return comboNN\n",
        "\n",
        "    elif \"ocNN\" in model_kind:\n",
        "        ########### Build \"overcomplicated\" model ocNN ########### \n",
        "        x = layers.Flatten()(input_layer)\n",
        "        x = layers.Dense(1000, activation='relu')(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "\n",
        "        x1 = layers.Dense(600, activation='relu')(x) \n",
        "        x1 = layers.Dropout(0.2)(x1)\n",
        "\n",
        "        x2 = layers.Dense(200, activation='relu')(x) \n",
        "        x2 = layers.Dropout(0.1)(x2)\n",
        "\n",
        "        y = layers.Conv2D(20, 2, padding='same', activation='relu')(input_layer)\n",
        "        y = layers.MaxPooling2D(2)(y)\n",
        "\n",
        "        y1 = layers.Conv2D(20, 3, activation='relu')(y) \n",
        "        y1 = layers.AveragePooling2D(2)(y1)\n",
        "        y1 = layers.Flatten()(y1)\n",
        "        y1 = layers.Dropout(0.2)(y1)\n",
        "\n",
        "        y2 = layers.Conv2D(20, 2, activation='relu')(y) \n",
        "        y2 = layers.Conv2D(20, 2, activation='relu')(y2)\n",
        "        y2 = layers.MaxPooling2D(2)(y2)\n",
        "        y2 = layers.Flatten()(y2)\n",
        "        y2 = layers.Dropout(0.1)(y2)\n",
        "\n",
        "        z1 = layers.Concatenate()([x1,y1])\n",
        "        z1 = layers.Dropout(0.3)(z1)\n",
        "\n",
        "        z2 = layers.Concatenate()([x2,y2])\n",
        "        z2 = layers.Dropout(0.1)(z2)\n",
        "\n",
        "        z = layers.Concatenate()([z1,z2])\n",
        "        z = layers.Dense(1000,activation=\"relu\")(z)\n",
        "        z = layers.Dropout(0.2)(z)\n",
        "\n",
        "        logits_layer = layers.Dense(100, activation='softmax')(z) \n",
        "        ocNN = Model(input_layer, logits_layer, name=\"ocNN_\"+noise)\n",
        "        print (\"built\", ocNN.name)\n",
        "        return ocNN\n",
        "    \n",
        "    else:\n",
        "        print (\"Could not find that kind of model.\")\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoJA28PLUsUx"
      },
      "source": [
        "DNN = buildModel((\"DNN\",\"no\"))\n",
        "tf.keras.utils.plot_model(DNN,show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kvzPo4UUdCd",
        "cellView": "form"
      },
      "source": [
        "#@title Compile and Train functions\n",
        "## Same training loop for all models, with same compiler too\n",
        "def train(model, traintuple, valtuple, epochs=epochs):\n",
        "    '''Train a model on the given sets of data\n",
        "        Params: the given model,\n",
        "                the train data as a tuple of x,y,\n",
        "                the test data as a tuple of x,y\n",
        "        Returns: a dictionary of metric values after each epoch of training\n",
        "    '''\n",
        "    (x_train, y_train) = traintuple\n",
        "    (x_val, y_val) = valtuple\n",
        "\n",
        "    history = model.fit(x=x_train,\n",
        "                        y=y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=valtuple,\n",
        "                        epochs=epochs,  \n",
        "                        verbose=1)   \n",
        "    return history.history\n",
        "print(\"loaded function train(model, traintuple, testuple, epochs=4)\")\n",
        "\n",
        "def compile_model(model):    \n",
        "    model.compile(  loss=\"sparse_categorical_crossentropy\",\n",
        "                    optimizer=Adam(lr=learning_rate),\n",
        "                    metrics=['acc'])\n",
        "    print (\"Compiled model\", model.name)\n",
        "print(\"loaded function compile_model(model)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu2_-h-uEfJ9"
      },
      "source": [
        "#################### Build all 20 models and store in dict get_model\n",
        "get_model = {}  # Training key --> model\n",
        "for key in training_keys:  # One training key for each model trained on each noise level.\n",
        "     model = buildModel(key)  \n",
        "     get_model.update({key:model}) # One model for each training key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP2VqeELLpGX"
      },
      "source": [
        "####################### Compile and Train all 20 models\n",
        "stats = {}\n",
        "for key in training_keys:    \n",
        "    nc = key[1]\n",
        "    model = get_model[key]\n",
        "    compile_model(model)\n",
        "    train_stats = train(model, train_data[nc], val_data[nc], epochs=epochs)\n",
        "    stats.update({key:train_stats})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m94196ctr4T"
      },
      "source": [
        "##Evaluation and Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT8BvV8N_6Lv"
      },
      "source": [
        "## Plot the training accuracy for all 20 models ####\n",
        "ncols = 5\n",
        "nrows = 4 #len(training_keys)//ncols\n",
        "y = len(training_keys)\n",
        "fig = plt.figure(figsize=(15,15)) \n",
        "axarr = fig.subplots(nrows,ncols,sharex=True,sharey=True,)\n",
        "axarr = axarr.reshape((20,))\n",
        "for i, tk in enumerate(training_keys):\n",
        "    ax = axarr[i]#fig.add_subplot(nrows,ncols,i+1)\n",
        "    ax.set_title(tk)\n",
        "    #ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_xlim(0,4)\n",
        "    #ax.set_ylabel(\"Accuracy\")\n",
        "    ax.set_ylim(0,1)\n",
        "    ax.plot(range(epochs),stats[tk]['acc'])\n",
        "    ax.plot(range(epochs),stats[tk]['val_acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w45M0fQcIk9E"
      },
      "source": [
        "_______________________"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTdOu7kTTneQ"
      },
      "source": [
        "############################# Evaluation: ALL MODELS, ALL NOISE LEVELS\n",
        "results = {}\n",
        "evalkeys = []\n",
        "for key in training_keys:\n",
        "    model = get_model[key]\n",
        "    print (\">>>>>>>>>>>>>>> Key:\", key)\n",
        "    for nl in noise_levels:\n",
        "        evalkey = (model.name, nl)\n",
        "        evalkeys += [evalkey]\n",
        "        x, y = test_data[nl]\n",
        "        evaluated = model.evaluate( x=x, y=y, verbose=0, batch_size=50) #500\n",
        "        results.update({evalkey:round(evaluated[1], 4)})\n",
        "        condition = noise2condition[nl]\n",
        "        print(model.name,\"tested on\",condition,\":\",round(evaluated[1], 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xztL8dDrYh-G"
      },
      "source": [
        "#models = model_kinds  #list(set(t[0] for t in training_keys))\n",
        "rows = len(training_keys)\n",
        "results_df = pd.DataFrame(columns=[\"model\"]+[\"trained_on\"]+noise_levels, index=range(rows))\n",
        "for row, (model_kind, train_condition) in enumerate(training_keys):\n",
        "    results_df.iloc[row][\"model\"]=model_kind\n",
        "    results_df.iloc[row][\"trained_on\"]=noise2condition[train_condition]\n",
        "    for test_condition in noise_levels:                 #TO DO:\n",
        "        results_df.iloc[row][test_condition] = results[(model_kind+\"_\"+train_condition, test_condition)]\n",
        "results_df = results_df.sort_values(\"model\")\n",
        "results_df = results_df.sort_values(\"trained_on\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyHdIHYbdO11"
      },
      "source": [
        "## Plot results for each model ####\n",
        "f = plt.figure(figsize=(18,18) )\n",
        "for i, m in enumerate(model_kinds):\n",
        "    modax = f.add_subplot(3,2,i+1)\n",
        "    modax.set(ylim=(0, 1))\n",
        "    \n",
        "    r_df = results_df.loc[lambda df: df[\"model\"]==m].sort_values('model')\n",
        "    r_df.plot(\"trained_on\", [\"no\",\"low\",\"high\",\"var\"], kind='bar', ax=modax)\n",
        "    \n",
        "    plt.title(\"Model \"+m)\n",
        "f.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TLOqVU4Lt23"
      },
      "source": [
        "f = plt.figure(figsize=(18,18))\n",
        "ax = []\n",
        "for i in range(4):\n",
        "    ax += [f.add_subplot(2,2,i+1)]#,label=nl)\n",
        "for i,nl in enumerate(noise_levels):\n",
        "    nl = noise2condition[nl]\n",
        "    r_df = results_df.loc[lambda df: df[\"trained_on\"]==nl].sort_values('model')\n",
        "    r_df.plot(\"model\", [\"no\",\"low\",\"high\",\"var\"], kind='bar', \n",
        "              ax=ax[i], \n",
        "              title=\"Trained on \"+nl, \n",
        "              ylabel=\"Evaluation (accuracy)\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}