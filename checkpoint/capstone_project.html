<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_qzr9e7f4x77r-6{list-style-type:none}ul.lst-kix_qzr9e7f4x77r-5{list-style-type:none}ul.lst-kix_qzr9e7f4x77r-4{list-style-type:none}ul.lst-kix_qzr9e7f4x77r-3{list-style-type:none}ul.lst-kix_qzr9e7f4x77r-2{list-style-type:none}ul.lst-kix_qzr9e7f4x77r-1{list-style-type:none}ul.lst-kix_qzr9e7f4x77r-0{list-style-type:none}.lst-kix_el517sh1wuno-1>li:before{content:"\0025cb  "}ul.lst-kix_ortl1148vb21-7{list-style-type:none}ul.lst-kix_ortl1148vb21-8{list-style-type:none}.lst-kix_el517sh1wuno-0>li:before{content:"\0025cf  "}ul.lst-kix_ortl1148vb21-0{list-style-type:none}ul.lst-kix_ortl1148vb21-1{list-style-type:none}ul.lst-kix_ortl1148vb21-2{list-style-type:none}ul.lst-kix_ortl1148vb21-3{list-style-type:none}ul.lst-kix_ortl1148vb21-4{list-style-type:none}ul.lst-kix_ortl1148vb21-5{list-style-type:none}ul.lst-kix_ortl1148vb21-6{list-style-type:none}ul.lst-kix_qzr9e7f4x77r-8{list-style-type:none}ul.lst-kix_qzr9e7f4x77r-7{list-style-type:none}.lst-kix_qzr9e7f4x77r-6>li:before{content:"\0025cf  "}.lst-kix_qzr9e7f4x77r-7>li:before{content:"\0025cb  "}.lst-kix_qzr9e7f4x77r-8>li:before{content:"\0025a0  "}ul.lst-kix_el517sh1wuno-4{list-style-type:none}ul.lst-kix_el517sh1wuno-5{list-style-type:none}ul.lst-kix_el517sh1wuno-2{list-style-type:none}ul.lst-kix_el517sh1wuno-3{list-style-type:none}ul.lst-kix_el517sh1wuno-8{list-style-type:none}ul.lst-kix_el517sh1wuno-6{list-style-type:none}ul.lst-kix_el517sh1wuno-7{list-style-type:none}ul.lst-kix_el517sh1wuno-0{list-style-type:none}ul.lst-kix_el517sh1wuno-1{list-style-type:none}.lst-kix_ortl1148vb21-6>li:before{content:"\0025cf  "}.lst-kix_ortl1148vb21-7>li:before{content:"\0025cb  "}.lst-kix_ortl1148vb21-8>li:before{content:"\0025a0  "}.lst-kix_qzr9e7f4x77r-5>li:before{content:"\0025a0  "}.lst-kix_qzr9e7f4x77r-4>li:before{content:"\0025cb  "}.lst-kix_qzr9e7f4x77r-3>li:before{content:"\0025cf  "}.lst-kix_qzr9e7f4x77r-1>li:before{content:"\0025cb  "}.lst-kix_el517sh1wuno-8>li:before{content:"\0025a0  "}.lst-kix_qzr9e7f4x77r-0>li:before{content:"\0025cf  "}.lst-kix_qzr9e7f4x77r-2>li:before{content:"\0025a0  "}.lst-kix_el517sh1wuno-7>li:before{content:"\0025cb  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_ortl1148vb21-5>li:before{content:"\0025a0  "}.lst-kix_ortl1148vb21-4>li:before{content:"\0025cb  "}.lst-kix_el517sh1wuno-5>li:before{content:"\0025a0  "}.lst-kix_el517sh1wuno-4>li:before{content:"\0025cb  "}.lst-kix_el517sh1wuno-6>li:before{content:"\0025cf  "}.lst-kix_ortl1148vb21-2>li:before{content:"\0025a0  "}.lst-kix_ortl1148vb21-3>li:before{content:"\0025cf  "}.lst-kix_ortl1148vb21-0>li:before{content:"\0025cf  "}.lst-kix_el517sh1wuno-2>li:before{content:"\0025a0  "}.lst-kix_el517sh1wuno-3>li:before{content:"\0025cf  "}.lst-kix_ortl1148vb21-1>li:before{content:"\0025cb  "}ol{margin:0;padding:0}table td,table th{padding:0}.c68{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1.5pt;border-right-width:1.5pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:38.2pt;border-top-color:#000000;border-bottom-style:solid}.c61{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c74{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1.5pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#000000;border-bottom-style:solid}.c46{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:300pt;border-top-color:#000000;border-bottom-style:solid}.c40{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#cccccc;border-left-width:2.2pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c9{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#cccccc;border-left-width:2.2pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c34{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c0{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#cccccc;border-left-width:2.2pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#000000;border-bottom-style:solid}.c51{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1.5pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:38.2pt;border-top-color:#cccccc;border-bottom-style:solid}.c49{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c64{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#cccccc;border-left-width:2.2pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c72{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:2.2pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c25{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1.5pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:38.2pt;border-top-color:#cccccc;border-bottom-style:solid}.c57{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:2.2pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c43{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c66{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1.5pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#cccccc;border-left-width:2.2pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#000000;border-bottom-style:solid}.c35{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1.5pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:38.2pt;border-top-color:#cccccc;border-bottom-style:solid}.c13{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c18{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:2.2pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c8{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1.5pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:38.2pt;border-top-color:#000000;border-bottom-style:solid}.c56{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:2.2pt;border-right-width:2.2pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#000000;border-bottom-style:solid}.c16{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#000000;border-bottom-style:solid}.c22{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:2.2pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c33{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:2.2pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#000000;border-bottom-style:solid}.c63{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c65{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1.5pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#000000;border-bottom-style:solid}.c36{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c50{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1.5pt;border-right-width:2.2pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#000000;border-bottom-style:solid}.c71{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#cccccc;border-left-width:2.2pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#000000;border-bottom-style:solid}.c54{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#000000;border-bottom-style:solid}.c42{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1.5pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:38.2pt;border-top-color:#cccccc;border-bottom-style:solid}.c31{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c52{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:2.2pt;border-right-width:1.5pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:38.2pt;border-top-color:#000000;border-bottom-style:solid}.c30{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#000000;border-bottom-style:solid}.c44{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#cccccc;border-left-width:2.2pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:75pt;border-top-color:#cccccc;border-bottom-style:solid}.c37{-webkit-text-decoration-skip:none;color:#000000;font-weight:700;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Georgia";font-style:normal}.c41{-webkit-text-decoration-skip:none;color:#000000;font-weight:700;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:14pt;font-family:"Georgia";font-style:normal}.c14{-webkit-text-decoration-skip:none;color:#999999;font-weight:700;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Georgia";font-style:normal}.c15{-webkit-text-decoration-skip:none;color:#000000;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Georgia"}.c19{color:#b7b7b7;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Georgia";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Georgia";font-style:normal}.c23{padding-top:4pt;padding-bottom:2pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Georgia";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c24{color:#999999;font-weight:400;vertical-align:baseline;font-size:12pt;font-family:"Georgia";font-style:normal}.c67{color:#000000;vertical-align:baseline;font-size:12pt;font-family:"Georgia";font-style:normal}.c58{color:#ff0000;font-weight:700;vertical-align:baseline;font-family:"Courier New";font-style:normal}.c53{padding-top:4pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left}.c48{border-spacing:0;border-collapse:collapse;margin-right:auto}.c11{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c69{color:#000000;font-weight:400;vertical-align:baseline;font-family:"Georgia"}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c28{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c17{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c29{font-size:10.5pt;font-family:"Courier New";font-weight:400}.c45{background-color:#ffffff;max-width:504pt;padding:72pt 54pt 72pt 54pt}.c60{padding:0;margin:0}.c27{color:inherit;text-decoration:inherit}.c10{margin-left:36pt;padding-left:0pt}.c38{width:33%;height:1px}.c47{font-family:"Arial";font-weight:700}.c21{color:#e69138;font-size:10pt}.c39{orphans:2;widows:2}.c2{height:15.8pt}.c20{text-decoration:none}.c73{height:27pt}.c7{height:12pt}.c26{font-size:10pt}.c70{color:#999999}.c12{font-style:italic}.c55{color:#0b5394}.c59{background-color:#ffff00}.c62{font-weight:700}.c32{color:#b7b7b7}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:12pt;font-family:"Georgia"}p{margin:0;color:#000000;font-size:12pt;font-family:"Georgia"}h1{padding-top:4pt;-webkit-text-decoration-skip:none;color:#000000;font-weight:700;text-decoration:underline;font-size:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-decoration-skip-ink:none;font-family:"Georgia";orphans:2;widows:2;text-align:left}h2{padding-top:4pt;-webkit-text-decoration-skip:none;color:#000000;font-weight:700;text-decoration:underline;font-size:12pt;padding-bottom:2pt;line-height:1.15;page-break-after:avoid;text-decoration-skip-ink:none;font-family:"Georgia";orphans:2;widows:2;text-align:left}h3{padding-top:4pt;-webkit-text-decoration-skip:none;color:#000000;text-decoration:underline;font-size:12pt;padding-bottom:2pt;line-height:1.15;page-break-after:avoid;font-style:italic;text-decoration-skip-ink:none;font-family:"Georgia";orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c45"><h1 class="c39 c53" id="h.mva5pdsz83qx"><span class="c41">I. Introduction</span></h1><h2 class="c23" id="h.vhztv3zudko"><span>Purpose</span></h2><p class="c1"><span>The purpose of this project is to</span><span>&nbsp;compare the performance of different neural network models on an image recognition task, under several levels of difficulty. &nbsp;The task will be to categorize images of handwritten double-digit numbers (00, 99) constructed from the classic single-digit MNIST database</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c6">&nbsp;(examples shown).</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 501.39px; height: 205.11px;"><img alt="" src="images/image5.png" style="width: 501.39px; height: 205.11px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Four models each of five neural network designs will be implemented for training on each of the four categories of &ldquo;noiseiness&rdquo;, for a total of 20 trained models to evaluate. &nbsp;Each will be evaluated on sets of test images; one set for each noise condition. The final results will then be a total of 80 accuracy measurements.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">The overall aim of the project will be to meaningfully compare model performance, between each other and between noise conditions. &nbsp;</span></p><p class="c1 c7"><span class="c6"></span></p><h2 class="c23" id="h.oqr2mb3jqrg7"><span class="c37">Questions and Expectations</span></h2><p class="c1"><span class="c6">The questions to be answered:</span></p><ul class="c60 lst-kix_el517sh1wuno-0 start"><li class="c1 c10 li-bullet-0"><span class="c6">Can the minimum model design do even better than chance? Even without image distortion? </span></li><li class="c1 c10 li-bullet-0"><span class="c6">How does DNN design differ from CNN (expectation: worse)</span></li><li class="c1 c10 li-bullet-0"><span class="c6">&nbsp;and then between those two and the miniNN, comboNN designs. (Does the comboNN do any better than the CNN? Expectation: no)</span></li><li class="c1 c10 li-bullet-0"><span class="c6">How much does training under a given condition affect performance? Between-models, which ones do best compared to the others and to themselves between training conditions?</span></li><li class="c1 c10 li-bullet-0"><span>How well will the ocNN perform compared to simpler DNN/CNN designs? (expectation: worse or at least more greedy)</span></li></ul><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><h1 class="c53 c39" id="h.225m5r2imom"><span>II. Dataset Construction</span></h1><h2 class="c23" id="h.sgm0nv5zslio"><span>Double-Digit Images</span></h2><p class="c1"><span>The data used for this evaluation will be derived from the canonical MNIST single-digit data.</span><sup><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span>&nbsp; Tensorflow</span><sup><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span class="c6">&nbsp;provides this data with a convenient-to-access built-in function, consisting of two sets of pixel arrays, 60,000 for training and 10,000 for testing. Both sets come from the same statistical distribution, according to the dataset designers. &nbsp;Each element is a 28x28 array of numbers ranging (0, 255) to be interpreted as values on a single greyscale color channel. These values represent handwritten images of a single digit, (0, 9). Paired with each image array is its corresponding label array with values (0,9) to indicate which digit the images are meant to represent.</span></p><p class="c1"><span class="c6">From this raw dataset of examples-and-labels, a new dataset of double-digit images, along with their labels, (0,99). &nbsp;</span></p><p class="c1"><span class="c6">To do so, I randomly selected a single-digit example to be the left-hand tens digit, and separately selected a random digit to be the right-hand ones digit. I then simply added these two arrays to each side of an empty array of 28x56 pixels. </span></p><p class="c1"><span class="c6">&nbsp;</span></p><p class="c1"><span class="c6">First I made sure to normalize each single digit image array from (0,255) down to (0,1), and it was important to do this before adding them to the new array, because each one had a different distribution of values (different means and standard deviations). &nbsp;Once normalized, they all had comparable brightness levels and ranges of grayscale values, and looked natural when combined (examples shown in Figure 1). &nbsp;</span></p><p class="c1"><span class="c6">There was some occasional overlap between the two digits, and since I was adding the two smaller arrays together into the large one, I also had to be careful to clip the resulting values at 1.0, and this may have altered the original distributions very slightly. &nbsp;Labels for these new examples were calculated as 10 times the tens digit plus the ones digit, so were in the range (0,99).</span></p><p class="c1"><span class="c6">I would use 100,000 of these new arrays (now normalized to the range (0, 1)), as a training set; another 5,000 image arrays on which to validate while training (ie. to be measured between epochs); and then another set of 10,000 double-digit images to be used for the final evaluation after training. &nbsp;<br>Drawing from the original 60,000 MNIST training images, the likelihood of an algorithm seeing the same double-digit image twice during training is around one in 36,000, which is sufficient to help prevent models from overfitting to the training dataset. &nbsp;</span></p><p class="c1"><span class="c6">The validation and evaluation double-digit sets were separately drawn at random from the original &nbsp;test-set of 10,000 single-digit images provided by MNIST. &nbsp;(This was to ensure neither digit of an image in the test set had also been present in the training set.)</span></p><p class="c1 c7"><span class="c6"></span></p><h2 class="c23" id="h.apr9yed1your"><span class="c37">Noise Conditions</span></h2><p class="c1"><span class="c6">Next, all of these images will have three different noise conditions applied to them: low noise, high noise, and variable noise. The &ldquo;noise&rdquo; was given random pixel values drawn from a normal distribution centered on the mean of the double-digit pixel values, with variation expressed in relation to the standard deviation of those original values. The low noise category could then be defined as 0.3 times the image SD. High noise was 1.3 times the SD. &nbsp;The variable category ranged in values from 0 to 1.3 times the SD, with no noise being zero noise added. (Note: at about twice the SD, images were noisy enough to be consistently indiscernible to a typical human test subject).</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span>The remaining difficulty in implementing the dataset construction is that all the images get normalized </span><span class="c12">twice</span><span class="c6">. &nbsp;The first normalization has to occur before the single-digit images are combined, because the distribution of pixel values differs greatly from one image array to the next. Then, they have to be normalized again after being combined, because the addition of noise de-normalizes them again. There is surely a better way to do this, but it will have to wait to be implemented in future investigations.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">The specific algorithms used in these constructions can be found in the project notebook, in the section entitled &ldquo;Construct the Data&rdquo;.</span></p><p class="c1 c7"><span class="c6"></span></p><h2 class="c23" id="h.9lno3t572r2b"><span class="c37">Preprocessing</span></h2><p class="c1"><span class="c6">The MNIST dataset is well-designed and much-used. &nbsp;Most of the heavy statistical preprocessing and normalization has already been done.</span></p><p class="c1"><span class="c6">Preprocess:</span></p><p class="c1"><span class="c6">Normalize pixel values, from (0, 255) to (0, 1), so that images with different distributions of pixel values can be meaningfully compared with each other.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Adding channel dim.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">*No* pre-reshaping for mini &amp; dense models</span></p><p class="c1 c7"><span class="c6"></span></p><h2 class="c23" id="h.6277e9ul9dcb"><span class="c37">Metrics</span></h2><p class="c1"><span>The metric for model performance will be </span><span class="c12">accuracy</span><span class="c6">, defined as the number of correct predictions made divided by the total number of predictions made by the model during testing.</span></p><p class="c1 c7"><span class="c6"></span></p><h3 class="c23" id="h.rxp4gu6qxtl0"><span class="c15 c12">Baselines</span></h3><p class="c1"><span>The </span><span class="c12">qualifying</span><span>&nbsp;or </span><span class="c12">threshold</span><span class="c6">&nbsp;baseline will be the accuracy expected for a model by chance alone; ie. its expected score if it were guessing uniformly at random, which in this case would be 0.01, or 1% (given the 100 possible answers, (0, 99)). &nbsp;Any functioning model with an accuracy above this bare minimum will be considered an acceptable algorithm, and worth comparing with the other models. If it can not achieve at least this level of accuracy, a redesign of that model type will have to be considered before it can be meaningfully compared to the other models and conditions.</span></p><p class="c1"><span>The </span><span class="c12">successful</span><span class="c6">&nbsp;baseline of a model&rsquo;s performance will be set at 90% accuracy. Any model performing at better than 90% will be considered a successful model. This is an arbitrary number and could be set differently (it should be noted that accuracies above 95% are easily achievable by most model designs tested on the original single-digit/no-noise dataset, depending on hyperparameter settings).</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span>The </span><span class="c12">key</span><span>&nbsp;or </span><span class="c12">benchmark</span><span class="c6">&nbsp;baseline for comparison will be the five model-types trained on images without any added noisiness. &nbsp;The performance of these trained models on the no-noise validation set will produce five accuracy measurements, which will then serve as a baseline for comparison with the other combinations of training/testing conditions, and with each other. The key baseline will also serve as a good benchmark for comparison (see next section).</span></p><p class="c1"><span class="c6">This baseline criterium might also reflect on the question of how well each variety of model performs on to discrimination of no-noise double-digit images compared to single-digits. (It would be interesting to run some comparisons between models&rsquo; performances on single- and double-digit images, but that will be outside the scope of the current project).</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span>Other metrics and baselines for comparison between models and conditions could be considered; for example, the training epochs required by each model-type to reach its asymptotic limit, or how the number of a model&rsquo;s trainable parameters affects its performance. However, these other metrics will also be outside the scope of this current project.</span></p><h2 class="c23 c7" id="h.d4wc8beyfoja"><span class="c37"></span></h2><h3 class="c23" id="h.ohq3oxlsc0us"><span class="c15 c12">Benchmarks</span></h3><p class="c1"><span class="c6">There are five different model types, so each type will require a benchmark model for comparison of model-type performance between noise conditions. These benchmarks will be a model of each type trained on the original no-noise images. Their performance on the no-noise test data will then be compared to that of models of the same type but trained on the other noise conditions. &nbsp;</span></p><p class="c1"><span class="c6">The baseline for noise-levels will be the no-noise condition.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><h2 class="c23" id="h.5hq9xrxvbyec"><span class="c37">Dataset Analysis</span></h2><p class="c1"><span class="c19">In this section, you will be expected to analyze the data you are using for the problem. This data can either be in the form of a dataset (or datasets), input data (or input files), or even an environment. The type of data should be thoroughly described and, if possible, have basic statistics and information presented (such as discussion of input features or defining characteristics about the input or environment). Any abnormalities or interesting qualities about the data that may need to be addressed have been identified (such as features that need to be transformed or the possibility of outliers). Questions to ask yourself when writing this section:</span></p><p class="c1"><span class="c32">- _If a dataset is present for this problem, have you thoroughly discussed certain features about the dataset? </span><span class="c62 c70">Has a data sample been provided to the reader?</span><span class="c24 c20">_</span></p><p class="c1"><span class="c19">- _If a dataset is present for this problem, are statistics about the dataset calculated and reported? Have any relevant results from this calculation been discussed?_</span></p><p class="c1"><span class="c19">- _If a dataset is **not** present for this problem, has discussion been made about the input space or input data for your problem?_</span></p><p class="c1"><span class="c19">- _Are there any abnormalities or characteristics about the input space or dataset that need to be addressed? (categorical variables, missing values, outliers, etc.)_</span></p><p class="c1 c7"><span class="c6"></span></p><h3 class="c23" id="h.hlg0rbxy3ty6"><span class="c15 c12">Example Set</span></h3><p class="c1"><span>The first row is a random sample of &nbsp;the original double-digit images, before adding any &ldquo;noise&rdquo; to them. &nbsp;The next three rows show the application to these images of the low, high, and variable noise levels (in that order). &nbsp;In the variable-noise dataset, each image had a different degree of noise added, ranging anywhere from </span><span class="c12">no</span><span>&nbsp;noise to </span><span class="c12">high</span><span class="c6">&nbsp;noise levels.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c62">Figure 1</span><span>. Samples of images with </span><span class="c12">no</span><span>, </span><span class="c12">low</span><span>, </span><span class="c12">high</span><span>, and </span><span class="c12">variable</span><span class="c6">&nbsp;noise added </span></p><p class="c1"><span class="c5">(In that order, by row. The first row is from the baseline dataset with no added noise.)</span></p><h2 class="c1" id="h.8au51sc8rvk2"><span class="c20">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 254.67px;"><img alt="" src="images/image3.png" style="width: 624.00px; height: 254.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h2><h2 class="c23 c7" id="h.l6n2xmu56vtf"><span class="c37"></span></h2><h3 class="c23" id="h.o4u6oely0ntd"><span class="c15 c12">Exploratory Visualization</span></h3><p class="c1"><span class="c19">In this section, you will need to provide some form of visualization that summarizes or extracts a relevant characteristic or feature about the data. The visualization should adequately support the data being used. Discuss why this visualization was chosen and how it is relevant. Questions to ask yourself when writing this section:</span></p><p class="c1"><span class="c19">- _Have you visualized a relevant characteristic or feature about the dataset or input data?_</span></p><p class="c1"><span class="c19">- _Is the visualization thoroughly analyzed and discussed?_</span></p><p class="c1"><span class="c19">- _If a plot is provided, are the axes, title, and datum clearly defined?_</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Histograms?</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><h3 class="c23" id="h.hufsvkil11fi"><span class="c15 c12">Statistics</span></h3><p class="c1"><span>This data would be well-suited to a </span><span class="c12">k</span><span>-fold cross validation</span><sup><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span class="c6">&nbsp;analysis. &nbsp;That was not possible to implement within the constraints of this project. &nbsp;For this project, statistical analysis will be largely absent, and comparisons will be considered only roughly descriptive.</span></p><p class="c1 c7"><span class="c6"></span></p><h1 class="c53 c39" id="h.ce5ycf8uor98"><span>III. Methodology</span></h1><h2 class="c23" id="h.mi9jh3umkyst"><span class="c37">Algorithms and Techniques</span></h2><p class="c1"><span>The models were implemented in python with the Keras </span><span class="c12">Functional API</span><sup><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span>&nbsp;on the TensorFlow</span><sup><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span class="c6">&nbsp;platform. &nbsp;The code developed for this implementation can be found in the Colab notebook included with this project.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">All the Keras layers were set to &nbsp;</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">simple model, CNN, DNN, combo, over-complicated</span></p><p class="c1"><span class="c6">One for each training noise level: no, low, high, and var = 20 models to train and evaluate</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><h3 class="c23" id="h.d28fk7cgvfwj"><span class="c12 c15">Hyperparameters</span></h3><p class="c1"><span class="c6">The training hyperparameters were set the same for every training session and every model. These were:</span></p><ul class="c60 lst-kix_qzr9e7f4x77r-0 start"><li class="c1 c10 li-bullet-0"><span class="c6">Batch size/Val-batch size: 500/100</span></li><li class="c1 c10 li-bullet-0"><span class="c6">Optimizer: RMSprop</span></li><li class="c1 c10 li-bullet-0"><span class="c6">Learning rate: 0.005</span></li><li class="c1 c10 li-bullet-0"><span class="c6">Sizes of training and validation sets: 100000 to train, 15000 to test</span></li><li class="c1 c10 li-bullet-0"><span class="c6">Epochs: 5</span></li><li class="c1 c10 li-bullet-0"><span class="c6">Image size: (28,56)</span></li></ul><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><h2 class="c23" id="h.lwvaekjjpxl8"><span class="c37">Implementation</span></h2><p class="c1"><span class="c24 c20">In this section, the process for which metrics, algorithms, and techniques that you implemented for the given data will need to be clearly documented. It should be abundantly clear how the implementation was carried out, and discussion should be made regarding any complications that occurred during this process. Questions to ask yourself when writing this section:</span></p><p class="c1"><span class="c24 c20">- _Is it made clear how the algorithms and techniques were implemented with the given datasets or input data?_</span></p><p class="c1"><span class="c24 c20">- _Were there any complications with the original metrics or techniques that required changing prior to acquiring a solution?_</span></p><p class="c1"><span class="c24 c20">- _Was there any part of the coding process (e.g., writing complicated functions) that should be documented?_</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Five kinds of neural network models will be designed and trained on the data, and then compared on the basis of their post-training performance on new, unseen data. The difficulty levels will be defined by four degrees of random &ldquo;noise&rdquo; added to the images: no noise, low noise, high noise, and variable noise. &nbsp;Each neural network will be trained on each noise condition, and then subsequently evaluated on each noise level so that the differences between models can be compared.</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 336.00px; height: 348.00px;"><img alt="" src="images/image6.png" style="width: 336.00px; height: 348.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c6">Five different model designs will be implemented: a minimal model (&ldquo;miniNN&rdquo;), essentially just a layer for input and a layer for output; a convolutional neural network (&ldquo;CNN&rdquo;), with only convolutional and pooling layers; a densely connected network (&ldquo;DNN&rdquo;), with only dense and dropout layers; a hybrid model made of both convolutional and dense layers (&ldquo;comboNN&rdquo;); and finally an &ldquo;overcomplicated&rdquo; model, with unnecessarily many dense and convolutional layers, dividing apart and concatenating together again along a whimsically complicated scheme of information-flow (&ldquo;ocNN&rdquo;) </span></p><p class="c1"><span class="c6">(Example shown; see Appendix A for a full set).</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Four models of each of these five design-types will be built to be trained on each of these four categories of noiseiness, for a total of 20 trained models. &nbsp;Each of these trained models will then be tested on validation sets of images, one for each noise condition, to yield a total of 80 accuracy measurements to consider.</span></p><p class="c1"><span class="c6">All layers to use ReLU activation function.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Also to consider, the params for each model type:</span></p><p class="c1"><span class="c6">miniNN&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;156,900</span></p><p class="c1"><span class="c6">CNN&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;236,730</span></p><p class="c1"><span class="c6">DNN &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1,933,831</span></p><p class="c1"><span class="c6">comboNN&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3,560,800</span></p><p class="c1"><span class="c6">ocNN&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6,397,860 </span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span>The python implementation of the neural network algorithms can be found &nbsp;in the project notebook under the heading, &ldquo;Build and Train the Models&rdquo;.</span></p><p class="c1 c7"><span class="c6"></span></p><h2 class="c23" id="h.2accfy2gzphk"><span>Obstacles and Adjustments</span></h2><p class="c1"><span class="c19">In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:</span></p><p class="c1"><span class="c19">- _Has an initial solution been found and clearly reported?_</span></p><p class="c1"><span class="c19">- _Is the process of improvement clearly documented, such as what techniques were used?_</span></p><p class="c1"><span class="c19">- _Are intermediate and final solutions clearly reported as the process is improved?_</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span>Not starting out with a </span><span class="c12">k</span><span class="c6">-fold cross validation analysis.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Only the DNN model had difficulty rising above the 1% accuracy to be expected by chance alone, under the training &nbsp;conditions. It seemed unable, through many runs, to learn any meaningful patterns under the high and the variable noise conditions. &nbsp;(The results reported here are from a typical run.)</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span>It is not yet clear how to explain these spurious results. &nbsp;The fact that this did not </span><span class="c12">always</span><span>&nbsp;occur suggests that it is not a fundamental problem with model design, or with the data itself. &nbsp;Trying different hyperparameters might solve this problem, or perhaps a different model design. &nbsp;Implementing a </span><span class="c12">k</span><span class="c6">-fold cross validation analysis might help to clarify why and how often these training failures occur.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">This choice of benchmark for model performance made it hard to tell any difference between them. &nbsp;It will still be considered worthwhile for approximate comparison with the same model&rsquo;s noise conditions.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><h1 class="c53 c39" id="h.wni80l1ixc4t"><span>IV. Results</span></h1><h2 class="c23" id="h.h1jfc4s4meot"><span class="c37">Training</span></h2><p class="c1"><span class="c6">Each table in Figure 2 displays the learning curve for one of the twenty models training on each of the different noise levels. &nbsp;The first row are the benchmark models: one of each type trained on images with no added noise distortion. &nbsp;These results show very little difference between model types under this condition.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c62">Figure 2.</span><span class="c6">&nbsp;Learning curves for each model type training on each noise condition.</span></p><p class="c1"><span class="c26">Labels are of the form</span><span class="c20 c26 c12 c69">&nbsp;(model-type, noise condition). </span></p><p class="c1"><span class="c26 c55">Blue line</span><span class="c5">: training accuracy at each epoch; </span></p><p class="c1"><span class="c21">Orange line</span><span class="c5">: validation accuracy at each epoch.</span></p><p class="c1 c7"><span class="c5"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 672.00px; height: 660.00px;"><img alt="" src="images/image4.png" style="width: 672.00px; height: 660.48px; margin-left: 0.00px; margin-top: -0.24px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">&nbsp;</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Figure 3 shows the evaluation results for each of the baseline models. &nbsp;What is immediately apparent is </span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 320.00px; height: 266.69px;"><img alt="" src="images/image7.png" style="width: 320.00px; height: 266.69px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c6">The first comparison to make with these values would be the results for the other training noise-level conditions as evaluated on the &ldquo;no noise&rdquo; dataset, too.</span></p><p class="c1 c7"><span class="c6"></span></p><h2 class="c23" id="h.in0rli4tb20x"><span>Evaluation</span></h2><p class="c1"><span class="c19">In this section, your model&rsquo;s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:</span></p><p class="c1"><span class="c19">- _Are the final results found stronger than the benchmark result reported earlier?_</span></p><p class="c1"><span class="c19">- _Have you thoroughly analyzed and discussed the final solution?_</span></p><p class="c1"><span class="c19">- _Is the final solution significant enough to have solved the problem?_</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Table 1. Accuracy for trained models under evaluation on each noise level</span></p><a id="t.1a38aa0f48d5d1c539d73b975bb32eb8e557551e"></a><a id="t.0"></a><table class="c48"><tbody><tr class="c73"><td class="c71" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c52" colspan="1" rowspan="1"><p class="c11"><span class="c3">Train</span></p></td><td class="c46" colspan="4" rowspan="1"><p class="c4"><span class="c47 c26">Test condition:</span></p></td></tr><tr class="c2"><td class="c40" colspan="1" rowspan="1"><p class="c4"><span class="c47 c26">Model</span></p></td><td class="c51" colspan="1" rowspan="1"><p class="c11"><span class="c3">cond:</span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c4"><span class="c47 c26">no</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c4"><span class="c47 c26">high</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c4"><span class="c47 c26">low</span></p></td><td class="c57" colspan="1" rowspan="1"><p class="c4"><span class="c26 c47">var</span></p></td></tr><tr class="c2"><td class="c66" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c68" colspan="1" rowspan="1"><p class="c4"><span class="c3">no</span></p></td><td class="c74" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.8618</span></p></td><td class="c65" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.8848</span></p></td><td class="c65" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.835</span></p></td><td class="c50" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.8748</span></p></td></tr><tr class="c2"><td class="c9" colspan="1" rowspan="1"><p class="c4"><span class="c47 c26">miniNN</span></p></td><td class="c42" colspan="1" rowspan="1"><p class="c4"><span class="c3">low</span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9286</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.935</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.6116</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.8976</span></p></td></tr><tr class="c2"><td class="c9" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c42" colspan="1" rowspan="1"><p class="c4"><span class="c3">high</span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9466</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.6202</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.023</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.403</span></p></td></tr><tr class="c2"><td class="c44" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c4"><span class="c3">var</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9324</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9396</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.8536</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9374</span></p></td></tr><tr class="c2"><td class="c0" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c3">no</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.904</span></p></td><td class="c54" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9164</span></p></td><td class="c54" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.863</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9108</span></p></td></tr><tr class="c2"><td class="c9" colspan="1" rowspan="1"><p class="c4"><span class="c47 c26">CNN</span></p></td><td class="c42" colspan="1" rowspan="1"><p class="c4"><span class="c3">low</span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9502</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9528</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.8058</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9458</span></p></td></tr><tr class="c2"><td class="c9" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c42" colspan="1" rowspan="1"><p class="c4"><span class="c3">high</span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9602</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9252</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.32</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.812</span></p></td></tr><tr class="c2"><td class="c44" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c4"><span class="c3">var</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9546</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9528</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.8714</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9468</span></p></td></tr><tr class="c2"><td class="c0" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c3">no</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.0368</span></p></td><td class="c54" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.0382</span></p></td><td class="c54" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.0204</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.023</span></p></td></tr><tr class="c2"><td class="c9" colspan="1" rowspan="1"><p class="c4"><span class="c47 c26">DNN</span></p></td><td class="c42" colspan="1" rowspan="1"><p class="c4"><span class="c3">low</span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.7484</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.7928</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.4212</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.7196</span></p></td></tr><tr class="c2"><td class="c9" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c42" colspan="1" rowspan="1"><p class="c4"><span class="c3">high</span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9266</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.2136</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.0176</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.2266</span></p></td></tr><tr class="c2"><td class="c44" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c4"><span class="c3">var</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.818</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.8278</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.6752</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.7928</span></p></td></tr><tr class="c2"><td class="c0" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c3">no</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.6758</span></p></td><td class="c54" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.699</span></p></td><td class="c54" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.6152</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.684</span></p></td></tr><tr class="c2"><td class="c9" colspan="1" rowspan="1"><p class="c4"><span class="c47 c26">comboNN</span></p></td><td class="c42" colspan="1" rowspan="1"><p class="c4"><span class="c3">low</span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.7296</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.7882</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.2762</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.6682</span></p></td></tr><tr class="c2"><td class="c9" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c42" colspan="1" rowspan="1"><p class="c4"><span class="c3">high</span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.8104</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.186</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.0128</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.1948</span></p></td></tr><tr class="c2"><td class="c44" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c4"><span class="c3">var</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.804</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.7926</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.5988</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.7546</span></p></td></tr><tr class="c2"><td class="c0" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c3">no</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9312</span></p></td><td class="c54" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.94</span></p></td><td class="c54" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9154</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9394</span></p></td></tr><tr class="c2"><td class="c9" colspan="1" rowspan="1"><p class="c4"><span class="c47 c26">ocNN</span></p></td><td class="c42" colspan="1" rowspan="1"><p class="c4"><span class="c3">low</span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9678</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9664</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.7258</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9516</span></p></td></tr><tr class="c2"><td class="c9" colspan="1" rowspan="1"><p class="c11 c7"><span class="c3"></span></p></td><td class="c42" colspan="1" rowspan="1"><p class="c4"><span class="c3">high</span></p></td><td class="c43" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9712</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9542</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.0178</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.6892</span></p></td></tr><tr class="c2"><td class="c64" colspan="1" rowspan="1"><p class="c7 c11"><span class="c3"></span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c4"><span class="c3">var</span></p></td><td class="c49" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9626</span></p></td><td class="c63" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9606</span></p></td><td class="c63" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.8886</span></p></td><td class="c72" colspan="1" rowspan="1"><p class="c4"><span class="c3">0.9574</span></p></td></tr></tbody></table><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">The baseline accuracy of models trained on no-noise images and evaluated on no-noise images is shown in Figure 3. &nbsp;Every model-type, except for the &ldquo;minimal&rdquo; baseline model, performed above the 90% accuracy threshold under these conditions. &nbsp;</span></p><h3 class="c23" id="h.eualvalyi3kh"><span class="c15 c12">Between-Models Comparisons</span></h3><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span>Figure 2(</span><span class="c12">a-d</span><span class="c6">): Evaluation results for each training condition, by model. </span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 636.86px; height: 499.22px;"><img alt="" src="images/image1.png" style="width: 636.86px; height: 499.22px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">The charts in this figure show the results for each model, by training condition.</span></p><p class="c1"><span class="c6">From Figure 2a (upper left) it is clear that none of the models trained on the no noise condition performed better under the other noise conditions. Models DNN, ocNN, and miniNN almost failed to pass the minimum benchmark of 0.01, with DNN and miniNN also showing difficulty with the &ldquo;low&rdquo; and &ldquo;var&rdquo; conditions, too. &nbsp;However the others did nearly as well with &ldquo;low&rdquo; noise as with none. The best all-around performer for this training condition was the comboNN model. </span></p><p class="c1"><span class="c6">Figures 2c and 2d (bottom row) are interesting: every model trained on &ldquo;high&rdquo; or &ldquo;variable&rdquo; noise performed about as well as on all the other evaluation conditions. &nbsp;This indicates a more generalized and robust representation of NNs trained on these conditions, as compared to those trained only on &ldquo;no&rdquo; or &nbsp;&ldquo;low&rdquo; noise levels.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span>&lt;&lt; Barchart for &ldquo;Model miniNN&rdquo; alone, trained/tested on each nc, as &ldquo;Figure 3</span><span class="c12">a</span><span class="c6">&rdquo;? &gt;&gt;</span></p><h3 class="c23" id="h.wmvxosttor99"><span>Between </span><span class="c15 c12">Noise-Conditions</span></h3><p class="c1"><span>Figure 3(</span><span class="c12">b-e</span><span class="c6">). Evaluation results for each model, by training condition.</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 568.94px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 568.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><h1 class="c53 c39" id="h.mh7qwpet8x0f"><span>V. Conclusion</span></h1><h2 class="c23" id="h.ifm33ypelbp6"><span class="c37">Discussion</span></h2><p class="c1"><span class="c19">In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:</span></p><p class="c1"><span class="c19">- _Have you thoroughly summarized the entire process you used for this project?_</span></p><p class="c1"><span class="c19">- _Were there any interesting aspects of the project?_</span></p><p class="c1"><span class="c19">- _Were there any difficult aspects of the project?_</span></p><p class="c1"><span class="c19">- _Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?_</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Maybe have done either doubling of digits OR noise, not both at same time? &nbsp;Because underlying question is how models compare, and no-noise was too easy to really distinguish?</span></p><p class="c1"><span class="c6">and</span></p><p class="c1 c7"><span class="c6"></span></p><h3 class="c23" id="h.12owx5s007u"><span class="c15 c12">Questions Answered</span></h3><p class="c1"><span class="c6">The questions that were answered were:</span></p><ul class="c60 lst-kix_el517sh1wuno-0"><li class="c1 c10 li-bullet-0"><span class="c6">Can the minimum model design do even better than chance? Even without image distortion? </span></li><li class="c1 c10 li-bullet-0"><span class="c6">How does DNN design differ from CNN (expectastion: worse)</span></li><li class="c1 c10 li-bullet-0"><span class="c6">&nbsp;and then between those two and the miniNN, comboNN designs. (Does the comboNN do any better than the CNN? Expectation: no)</span></li><li class="c1 c10 li-bullet-0"><span class="c6">How much does training under a given condition affect performance? Between-models, which ones do best compared to the others and to themselves between training conditions?</span></li><li class="c1 c10 li-bullet-0"><span>How well will the ocNN perform compared to simpler DNN/CNN designs? (expectation: worse or at least more greedy)</span></li></ul><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><h3 class="c23" id="h.f599v8nnydgr"><span class="c15 c12">Free-Form Visualization</span></h3><p class="c1"><span class="c19">In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:</span></p><p class="c1"><span class="c19">- _Have you visualized a relevant or important quality about the problem, dataset, input data, or results?_</span></p><p class="c1"><span class="c19">- _Is the visualization thoroughly analyzed and discussed?_</span></p><p class="c1"><span class="c32">- _If a plot is provided, are the axes, title, and datum clearly defined?_</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Maybe graphs of each model compared to its baseline?</span></p><p class="c1 c7"><span class="c6"></span></p><h2 class="c23" id="h.ojpdpqkhu4jl"><span>For Further Study</span></h2><p class="c1"><span class="c24 c20">In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:</span></p><p class="c1"><span class="c24 c20">- _Are there further improvements that could be made on the algorithms or techniques you used in this project?_</span></p><p class="c1"><span class="c24 c20">- _Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?_</span></p><p class="c1"><span class="c20 c24">- _If you used your final solution as the new benchmark, do you think an even better solution exists?_</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Implementation of k-means.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">What about more noise? &nbsp;Can they do better than humans above a certain noise threshold?</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c59">The DNN model</span><span class="c6">&nbsp;could be redesigned to not fail the minimum accuracy baseline of 0.01 so often. &nbsp;Are there any parameters or settings that would help this model learn under the high noise condition? &nbsp;What about another parallel sequence of dense layers or two? What approaches are there in the literature involving dense networks and image discrimination tasks? &nbsp;Would more layers or neurons help any, or is this design just not suited for this task?</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">It would be nice to experiment with different hyperparameters, including learning rate and number of training epochs. &nbsp;</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Also, which size dataset is sufficient for all the models to train successfully, if any? Smaller, larger?</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Noisier dataset, eg. SAT-6.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">Make a less-complicated ocNN. &nbsp;Different levels of design complication.</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c59">&ldquo;There was some occasional overlap between the two digits, and since I was adding the two smaller arrays together into the large one, I also had to be careful to clip the resulting values at 1&rdquo;</span><span>&nbsp; could be changed to assignment (0,1.0). &nbsp;&larr; but that would skew the distribution way off</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c59">The baseline accuracy of models trained on no-noise images and evaluated on no-noise images is shown in Figure X</span><span class="c6">&nbsp;FIX problem with constructing these: don&rsquo;t send no-noise to get_noise()</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c6">-----------</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1"><span class="c58 c20 c26">**Before submitting, ask yourself. . .**</span></p><p class="c1 c7"><span class="c58 c20 c26"></span></p><p class="c1"><span class="c58 c20 c26">- Does the project report you&rsquo;ve written follow a well-organized structure similar to that of the project template?</span></p><p class="c1"><span class="c58 c20 c26">- Is each section (particularly **Analysis** and **Methodology**) written in a clear, concise and specific fashion? Are there any ambiguous terms or phrases that need clarification?</span></p><p class="c1"><span class="c58 c20 c26">- Would the intended audience of your project be able to understand your analysis, methods, and results?</span></p><p class="c1"><span class="c58 c20 c26">- Have you properly proof-read your project report to assure there are minimal grammatical and spelling mistakes?</span></p><p class="c1"><span class="c58 c20 c26">- Are all the resources used for this project correctly cited and referenced?</span></p><p class="c1"><span class="c20 c26 c58">- Is the code that implements your solution easily readable and properly commented?</span></p><p class="c1"><span class="c58 c20 c26">- Does the code execute without error and produce results similar to those reported?</span></p><p class="c1 c7"><span class="c6"></span></p><p class="c1 c7"><span class="c6"></span></p><hr class="c38"><div><p class="c17 c39"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c26">&nbsp;</span><span class="c28 c26"><a class="c27" href="https://www.google.com/url?q=http://yann.lecun.com/exdb/mnist/&amp;sa=D&amp;source=editors&amp;ust=1619300187251000&amp;usg=AOvVaw2j2XjZTeHVygkHtwEexjmb">Official MNIST website</a></span></p></div><div><p class="c17 c39"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c26">&nbsp;</span><span class="c28 c26"><a class="c27" href="https://www.google.com/url?q=https://www.tensorflow.org/datasets/catalog/mnist&amp;sa=D&amp;source=editors&amp;ust=1619300187252000&amp;usg=AOvVaw2jCPG4UqS7sn3YCMKXTmm3">Tensorflow Dataset Catalogue</a></span><span class="c5">&nbsp;</span></p></div><div><p class="c17 c39"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c26">&nbsp;</span><span class="c28 c26"><a class="c27" href="https://www.google.com/url?q=https://www.tensorflow.org/datasets/add_dataset&amp;sa=D&amp;source=editors&amp;ust=1619300187253000&amp;usg=AOvVaw3noBpis3TkISUEj3Ux-oBO">MNIST Dataset Customization</a></span><span class="c26">&nbsp;specifically: </span><span class="c29">tf.keras.datasets.mnist.load_data()</span></p></div><div><p class="c17 c39"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c26">&nbsp;</span><span class="c28 c26"><a class="c27" href="https://www.google.com/url?q=https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification&amp;sa=D&amp;source=editors&amp;ust=1619300187251000&amp;usg=AOvVaw2r9AUAjK_Y3I8FGUksHFtr">&ldquo;</a></span><span class="c28 c26 c12"><a class="c27" href="https://www.google.com/url?q=https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification&amp;sa=D&amp;source=editors&amp;ust=1619300187252000&amp;usg=AOvVaw03sEPUeF_0wLkHXIkMAzYW">How to Develop a CNN for MNIST Handwritten Digit Classification</a></span><span class="c28 c26"><a class="c27" href="https://www.google.com/url?q=https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification&amp;sa=D&amp;source=editors&amp;ust=1619300187252000&amp;usg=AOvVaw03sEPUeF_0wLkHXIkMAzYW">&rdquo;</a></span><span class="c26">;</span><span class="c5">&nbsp;Jason Brownlee, Deep Learning for Computer Vision, May 8, 2019</span></p></div><div><p class="c17 c39"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c26">&nbsp;</span><span class="c28 c26 c12"><a class="c27" href="https://www.google.com/url?q=https://keras.io/guides/functional_api/&amp;sa=D&amp;source=editors&amp;ust=1619300187251000&amp;usg=AOvVaw18USIsJAWGE0mp_vnj27wQ">The Functional API</a></span><span class="c5">&nbsp;at Keras</span></p></div><div><p class="c17 c39"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c26">&nbsp;</span><span class="c26 c28"><a class="c27" href="https://www.google.com/url?q=https://www.tensorflow.org/&amp;sa=D&amp;source=editors&amp;ust=1619300187253000&amp;usg=AOvVaw0q259P0kKxHszkFyX45cbp">TensorFlow platform</a></span></p></div></body></html>