{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digits_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHK6DyunSbs4"
      },
      "source": [
        "# Machine learning algorithms for recognizing handwritten double-digit images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89yXy-JjuNgy"
      },
      "source": [
        "##Import the modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLy3pthUS0D2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random as rd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        " \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7WCZJZnuEqx"
      },
      "source": [
        "## Set the Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8Y3AFF5pgJZ"
      },
      "source": [
        "### Load raw MNIST data as tuples of numpy arrays\n",
        "### Each tuple is: (examples of images, corresponding labels)\n",
        "traintuple, testuple = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "### Set values for data construction\n",
        "image_size = (28,56) # base shape of a target image\n",
        "model_kinds = [\"miniNN\", \"CNN\", \"DNN\", \"comboNN\", \"ocNN\"]\n",
        "noise_levels = [\"no\", \"high\", \"low\", \"var\"]\n",
        "training_keys = []\n",
        "for nc in noise_levels:\n",
        "    for k in model_kinds:\n",
        "        training_keys.append((k, nc))\n",
        "\n",
        "### Dict to translate noise level to description\n",
        "describe_nl = {nc:d for (nc, d) in zip(noise_levels,\n",
        "                    [\"No noise\", \"High noise\", \"Low noise\", \"Variable noise\"])} \n",
        "\n",
        "### Number of images to generate for each category\n",
        "N_train = 100000    # for training\n",
        "N_test = 20000      # for post-training evaluation\n",
        "N_val = 10000       # to see progress while training\n",
        "\n",
        "### Training hyperparameters (same for all models)\n",
        "batch_size = 500\n",
        "validation_batch_size = 100\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "input_layer = layers.Input(shape=(28, 56, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY6KJV6z6l7_"
      },
      "source": [
        "# Construct the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW-Qn7I3QrZQ"
      },
      "source": [
        "### Construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X6jzIZm1vDZ"
      },
      "source": [
        "############ Define Functions for data construction\n",
        "\n",
        "def scale_array(arr):\n",
        "    '''Scale an array of numbers.\n",
        "        Params: numpy array of values to be scaled\n",
        "        Returns: the same array with values scaled to (0,1)\n",
        "    '''\n",
        "    max = np.max(arr)\n",
        "    min = np.min(arr)\n",
        "    scaled = (arr - min)/(max - min)\n",
        "    return scaled\n",
        "print(\"Loaded function scale_array(arr)\")\n",
        "\n",
        "def get_noise(image, noise_condition=\"no\"):\n",
        "    ''' Add a normal disitribution of noise to an image\n",
        "        Params: image: numpy array with values (0,1)\n",
        "                noise_factor: one of \"no\", \"low\", \"high\", \"var\"\n",
        "        Returns: noisy image array re-normalized to (0,1)\n",
        "    '''\n",
        "    # Dict to translate condition in terms of an image's standard deviation\n",
        "    cond2num = {\"no\":0, \n",
        "                \"low\":0.5, \n",
        "                \"high\":1.5, \n",
        "                \"var\":1.5*rd.random()} # Different value each call, (0,1.3*std)\n",
        "    noise_factor = np.std(image)*cond2num[noise_condition]\n",
        "    noise = np.random.normal(np.mean(image), \n",
        "                             noise_factor, \n",
        "                             size=image.shape)\n",
        "    # RE-normalize combined image back to (0,1)\n",
        "    return scale_array(noise + image)\n",
        "print(\"Loaded function get_noise(image, noise_condition)\")\n",
        "\n",
        "def doubleDigits(datatuple, nc=\"no\"):\n",
        "    ''' Merge two single-digit images into one double-digit image.\n",
        "        Params: images: np.array with shape (N,28,28), values 0 to 255\n",
        "                answers: np.array with shape (N,), values 0 to 9\n",
        "                nf: \"noise factor\" (float) is multiples/fractions of image std\n",
        "        Returns: noisy double-digit image as a numpy array with shape (28,56),\n",
        "                with values normed to (0.0, 1.0); \n",
        "                and the corresponding label, with values now 0 to 99\n",
        "    '''\n",
        "    (images,answers)=datatuple  # = (x, y)\n",
        "    \n",
        "    # Randomly select left and right single digit images\n",
        "    # with values 0 to 9, from the same raw training or test data set\n",
        "    left_index = rd.randrange(0, len(answers))   \n",
        "    right_index = rd.randrange(0, len(answers))\n",
        "    # Calculate double digit label 0 to 99\n",
        "    answer = answers[left_index]*10 + answers[right_index]\n",
        "    \n",
        "    # Have to scale them here because they have\n",
        "    # different distributions of pixel values\n",
        "    left_scaled = scale_array(images[left_index])\n",
        "    right_scaled = scale_array(images[right_index])\n",
        "\n",
        "    # Make background array with shape (28,56)\n",
        "    image = np.zeros(image_size)\n",
        "    # Group digits closer to middle of new image\n",
        "    width = image_size[1]\n",
        "    half_width = width//2  \n",
        "    # Shift left digit to the right\n",
        "    image[:,8:half_width+4] = left_scaled[:,4:half_width]\n",
        "    # Shift right digit to the left\n",
        "    image[:,half_width-4:width-8] += right_scaled[:,0:half_width-4]\n",
        "    # Have to clip in case some bright pixels overlap\n",
        "    image = image.clip(0,1)\n",
        "    # Add noise to the new double digit image\n",
        "    # get_noise() will overlay a Normal distribution of\n",
        "    # random pixel values centered on the mean of pixel values\n",
        "    # in the image, and with \"noise_factor\" as the width of the\n",
        "    # distribution in multiples or fractions of the \n",
        "    # standard deviation of the pixel values in the image:\n",
        "    # 0 = no distortion, +inf = uniform distribution  \n",
        "    # Rescale pixel values to (0,1) (again!)\n",
        "    image = get_noise(image, nc)  \n",
        "    return image, int(answer)              \n",
        "print(\"Loaded function doubleDigits(datatuple, nc=no)\")\n",
        "\n",
        "def getDoubleDigits(datatuple, how_many=1, nc=\"no\"):\n",
        "    ''' Aggregate a specified number of two-digit images, with or without noise\n",
        "        Params: image array of size (N, (image size)),\n",
        "                answers array of size (N,)\n",
        "        Returns: a single 28x56 double-digit image and\n",
        "                 the corresponding array of int labels\n",
        "    '''\n",
        "    yy = np.zeros((how_many,),dtype=int)\n",
        "    xx = np.zeros((how_many, image_size[0], image_size[1]))\n",
        "    for i in range(how_many):\n",
        "        dd, ans = doubleDigits(datatuple, nc)\n",
        "        yy[i] = ans\n",
        "        xx[i] = dd\n",
        "    return (xx, yy) # (new image, corresponding label)\n",
        "print(\"Loaded function getDoubleDigits(datatuple,how_many=1,nc=no)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YQljE-wizDw"
      },
      "source": [
        "### Generate sets of double-digit images for training, testing, and validation\n",
        "### One set for each noise level: no, low, high, and var\n",
        "\n",
        "train_data = {} # {noise level : tuple of training data (x,y)}\n",
        "val_data = {}   # {noise level : val data (x,y)}\n",
        "test_data = {}  # {noise level : test data (x,y)}\n",
        "\n",
        "for nc in noise_levels:\n",
        "    print (\"Noise level:\", describe_nl[nc])\n",
        "    if nc is \"no\":\n",
        "        ### Make a set of training, validation, and test images\n",
        "        ### First set with no noise added as a baseline\n",
        "        # Training set constructed from training set in MNIST\n",
        "        x_train, y_train = getDoubleDigits(traintuple, N_train, nc=nc)\n",
        "        print(\"Made\",N_train,\"new double-digit images to train on.\")\n",
        "        # Test and validation sets constructed from test images in MNIST\n",
        "        x_val, y_val = getDoubleDigits(testuple, N_val, nc=nc)\n",
        "        print(\"Made\",N_val,\"new double-digit images to validate on.\")\n",
        "        x_test, y_test = getDoubleDigits(testuple, N_test, nc=nc)\n",
        "        print(\"Made\",N_test,\"new double-digit images to test on.\")\n",
        "    else:\n",
        "        # Add noise to the baseline \"no noise\" set \n",
        "        x0_train, y_train = train_data[\"no\"]\n",
        "        x0_val, y_val = val_data[\"no\"]\n",
        "        x0_test, y_test = test_data[\"no\"]\n",
        "        x_train = np.array([get_noise(img.reshape(image_size),nc) for img in x0_train])\n",
        "        print(\"Added\",describe_nl[nc],\"to\",N_train,\"double-digit images for training.\")\n",
        "        x_val = np.array([get_noise(img.reshape(image_size),nc) for img in x0_val])\n",
        "        print(\"Added\",describe_nl[nc],\"to\",N_val,\"double-digit images for validation.\")\n",
        "        x_test = np.array([get_noise(img.reshape(image_size),nc) for img in x0_test])\n",
        "        print(\"Added\",describe_nl[nc],\"to\",N_test,\"double-digit images for evaluation.\")        \n",
        "    \n",
        "    ### Add a black-and-white channel dimension\n",
        "    x_train = x_train[..., np.newaxis].astype(\"float32\")\n",
        "    x_val = x_val[..., np.newaxis].astype(\"float32\")\n",
        "    x_test = x_test[..., np.newaxis].astype(\"float32\")\n",
        "\n",
        "    ### Update dict for converting from noiselevel to dataset\n",
        "    train_data.update({nc:(x_train, y_train)})\n",
        "    val_data.update({nc:(x_val, y_val)}) \n",
        "    test_data.update({nc:(x_test, y_test)})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBBGZTy4qFrJ"
      },
      "source": [
        "### Data distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tmD7E3wqlfj"
      },
      "source": [
        "# Display the distribution of pixel values for each noise level on a random image\n",
        "idx = rd.randrange(val_data[\"no\"][0].shape[0])\n",
        "f = plt.figure(figsize=(20,6))\n",
        "histax = f.subplots(2,4)\n",
        "for i, nl in enumerate(noise_levels):\n",
        "    xt = pd.DataFrame(val_data[nl][0].reshape(-1,28*56)).iloc[idx]\n",
        "    histax[0,i].set_title(describe_nl[nl])\n",
        "    histax[0,i].imshow(np.array(xt).reshape(image_size), )\n",
        "    xt.hist(ax=histax[1,i])\n",
        "    histax[1,i].set_ylim(0,700)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2rxp_Hoqd1p"
      },
      "source": [
        "### Sample set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiFcn-Bm4dnX"
      },
      "source": [
        "### Show a sample of five images for each noise level\n",
        "ncols = 5\n",
        "x0, y0 = getDoubleDigits(traintuple, nc=\"no\", how_many=ncols)\n",
        "print('Answers:',y0) \n",
        "\n",
        "plt.set_cmap('binary')\n",
        "f,a = plt.subplots(4,5,True,True,)\n",
        "f.set_size_inches(18,8)\n",
        "a=a.reshape(20,)\n",
        "for i, nl in enumerate(noise_levels):\n",
        "    if nl == \"no\": \n",
        "        x = x0\n",
        "    else: \n",
        "        x = np.array([get_noise(img.reshape(image_size),nl) for img in x0])    \n",
        "    for j in range(5): \n",
        "        if j==0: a[5*i].set_ylabel(describe_nl[nl])\n",
        "        if i==0: a[j].set_title(y0[j])\n",
        "        a[5*i+j].imshow(x[j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oqBkNBJmtUv"
      },
      "source": [
        "# Build and Train the Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtngjIXgUvws"
      },
      "source": [
        "### Define functions for building, training, compiling and evaluating models\n",
        "\n",
        "def build_model(key, verbose=True):\n",
        "    '''Builds a model of each kind, for each noise level\n",
        "        params: a tuple of \"model kind\" and \"noise level\"\n",
        "                verbose turns on printed reports\n",
        "        returns: a model of the specified kind, to be\n",
        "                 dedicated to noise of the specified level\n",
        "    '''\n",
        "    (model_kind, noise) = key\n",
        "    ## Input layer: same for for all models\n",
        "    input_layer = layers.Input(shape=(image_size[0], image_size[1], 1))\n",
        "        \n",
        "    if \"miniNN\" in model_kind:\n",
        "        ########### Build model miniNN as a minimal, baseline model \n",
        "        x = layers.Flatten()(input_layer)#\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        miniNN = Model(input_layer, output_layer, name=\"miniNN_\"+noise)\n",
        "        if verbose: print (\"built\", miniNN.name)\n",
        "        return miniNN\n",
        "\n",
        "    elif \"CNN\" in model_kind:\n",
        "        ########### Build model CNN: typical convolutional neural network\n",
        "        x = layers.Conv2D(20, 3, activation='relu')(input_layer)#, padding='same'\n",
        "        x = layers.MaxPooling2D(2)(x)\n",
        "        x = layers.Conv2D(30, 2, activation='relu')(x) \n",
        "        x = layers.AveragePooling2D(2)(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        CNN = Model(input_layer, output_layer, name=\"CNN_\"+noise)\n",
        "        if verbose: print (\"built\", CNN.name)\n",
        "        return CNN\n",
        "    \n",
        "    elif \"DNN\" in model_kind:\n",
        "        ########### Build model DNN, a densely connected NN with 4 layers\n",
        "        x = layers.Flatten()(input_layer)\n",
        "        x = layers.Dropout(0.1)(x)\n",
        "        x = layers.Dense(555, activation='relu')(x)\n",
        "        x = layers.Dropout(0.1)(x)\n",
        "        x = layers.Dense(222, activation='relu')(x)\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        DNN = Model(input_layer, output_layer, name=\"DNN_\"+noise)\n",
        "        if verbose: print (\"built\", DNN.name)\n",
        "        return DNN\n",
        "\n",
        "    elif \"comboNN\" in model_kind:\n",
        "        ########### Build model comboNN, a hybrid of both convolutional and dense layers \n",
        "        x = layers.Conv2D(20, 3, activation='relu')(input_layer)\n",
        "        x = layers.AveragePooling2D(2)(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dense(720, activation='relu')(x)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        comboNN = Model(input_layer, output_layer, name=\"comboNN_\"+noise)\n",
        "        if verbose: print (\"built\", comboNN.name)\n",
        "        return comboNN\n",
        "\n",
        "    elif \"ocNN\" in model_kind:\n",
        "        ########### Build \"overcomplicated\" model ocNN, of superfluous complexity \n",
        "        x = layers.Flatten()(input_layer)\n",
        "        x = layers.Dense(800, activation='relu')(x)\n",
        "\n",
        "        x1 = layers.Dropout(0.3)(x)\n",
        "        x1 = layers.Dense(500, activation='relu')(x1) \n",
        "        x1 = layers.Dropout(0.25)(x1)\n",
        "\n",
        "        x2 = layers.Dropout(0.1)(x)\n",
        "        x2 = layers.Dense(300, activation='relu')(x2) \n",
        "\n",
        "        y1 = layers.Conv2D(25, 5, activation='relu')(input_layer)\n",
        "        y1 = layers.MaxPooling2D(2)(y1)\n",
        "        y1 = layers.Conv2D(25, 3, activation='relu')(y1) \n",
        "        y1 = layers.AveragePooling2D(2)(y1)\n",
        "        y1 = layers.Conv2D(25, 2, activation='relu', padding='same')(y1)\n",
        "        y1 = layers.Flatten()(y1)\n",
        "        y1 = layers.Dropout(0.2)(y1)\n",
        "        \n",
        "        y2 = layers.Conv2D(22, 3, activation='relu')(input_layer) \n",
        "        y2 = layers.Conv2D(33, 2, activation='relu', padding='same')(y2)\n",
        "        y2 = layers.MaxPooling2D(2)(y2)\n",
        "        y2 = layers.Conv2D(11, 2, activation='relu')(y2)\n",
        "        y2 = layers.Flatten()(y2)\n",
        "        \n",
        "        z1 = layers.Concatenate()([x1,y1])\n",
        "        z1 = layers.Dropout(0.2)(z1)\n",
        "\n",
        "        z2 = layers.Concatenate()([x2,y2])\n",
        "        z2 = layers.Dropout(0.1)(z2)\n",
        "\n",
        "        z = layers.Dropout(0.4)(x)\n",
        "        z = layers.Concatenate()([z,z1,z2])\n",
        "        z = layers.Dropout(0.3)(z)\n",
        "        z = layers.Dense(2222,activation=\"relu\")(z)\n",
        "        z = layers.Dropout(0.2)(z)\n",
        "        z = layers.Dense(222,activation=\"relu\")(z)\n",
        "        \n",
        "        logits_layer = layers.Dense(100, activation='softmax')(z) \n",
        "        ocNN = Model(input_layer, logits_layer, name=\"ocNN_\"+noise)\n",
        "        if verbose: print (\"built\", ocNN.name)\n",
        "        return ocNN\n",
        "\n",
        "    else:\n",
        "        print (\"Could not find that kind of model.\")\n",
        "        return None\n",
        "print(\"loaded function build_model((model_kind, noise_level))\")\n",
        "\n",
        "## Same training loop for all models\n",
        "def train(model, traintuple, valtuple, epochs=epochs):\n",
        "    '''Train a model on the given sets of data\n",
        "        Params: the given model,\n",
        "                the train data as a tuple of x,y,\n",
        "                the test data as a tuple of x,y\n",
        "        Returns: a dictionary of metric values after each epoch of training\n",
        "    '''\n",
        "    (x_train, y_train) = traintuple\n",
        "    (x_val, y_val) = valtuple\n",
        "\n",
        "    history = model.fit(x=x_train,\n",
        "                        y=y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=valtuple,\n",
        "                        validation_batch_size=validation_batch_size,\n",
        "                        epochs=epochs,  \n",
        "                        verbose=1)   \n",
        "    return history.history\n",
        "print(\"loaded function train(model, traintuple, testuple, epochs=epochs)\")\n",
        "\n",
        "## All models are compiled the same\n",
        "def compile_model(model):    \n",
        "    model.compile(  loss=\"sparse_categorical_crossentropy\",\n",
        "                    optimizer=RMSprop(lr=learning_rate),                    \n",
        "                    metrics=['acc'])\n",
        "    print (\"Compiled model\", model.name)\n",
        "print(\"loaded function compile_model(model)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP2VqeELLpGX"
      },
      "source": [
        "### Build all 20 models\n",
        "get_model = {}  # Training key --> model\n",
        "for key in training_keys:  # One training key for each model on each noise level.\n",
        "     model = build_model(key, verbose=False)  \n",
        "     get_model.update({key:model}) # One model for each training key\n",
        "\n",
        "######### Print out summary tables for all the models\n",
        "for m in model_kinds:\n",
        "    print(get_model[(m,\"no\")].summary(),\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-Igt5s9-okJ"
      },
      "source": [
        "######## Plot model diagrams\n",
        "#tf.keras.utils.plot_model(get_model[(\"ocNN\",\"no\")], \n",
        "#                          show_layer_names=True, \n",
        "#                          show_shapes=True, \n",
        "#                          to_file=\"diagram_ocNN.png\")\n",
        "\n",
        "tf.keras.utils.plot_model(get_model[(\"comboNN\",\"no\")], \n",
        "                          show_layer_names=True, \n",
        "                          show_shapes=True, \n",
        "                          to_file=\"diagram_comboNN.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6XfSpmr6X2n"
      },
      "source": [
        "############# Compile and Train all 20 models\n",
        "stats = {}\n",
        "for key in training_keys:    \n",
        "    nc = key[1]\n",
        "    model = get_model[key]\n",
        "    compile_model(model)\n",
        "    train_stats = train(model, train_data[nc], val_data[nc], epochs=epochs)\n",
        "    stats.update({key:train_stats})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m94196ctr4T"
      },
      "source": [
        "#Evaluate the Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWlI_VvyRE67"
      },
      "source": [
        "### Training stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT8BvV8N_6Lv"
      },
      "source": [
        "### Plot the learning curves for all 20 models during training\n",
        "ncols = 5 #number of model types\n",
        "nrows = 4 #number of noise conditions\n",
        "\n",
        "fig = plt.figure(figsize=(16,16))\n",
        "fig.suptitle(\"Learning Curves: Accuracy by Epoch for each (model, noise level)\",va='bottom',fontsize=18)\n",
        "axarr = fig.subplots(nrows,ncols,sharey=True,sharex=True,)\n",
        "axarr = axarr.reshape((nrows*ncols,))\n",
        "\n",
        "for i, tk in enumerate(training_keys):\n",
        "    ax = axarr[i]\n",
        "    if i%ncols==0: ax.set_ylabel(\"Tested on \"+describe_nl[tk[1]])\n",
        "    if i>=15: ax.set_xlabel(\"Epochs\")   \n",
        "    ax.set_title(tk)\n",
        "    ax.set_xlim(1,epochs)\n",
        "    ax.set_ylim(0,1)\n",
        "    ax.set_yticks=[0.01]+[t/10 for t in range(1,10,2)]+[0.95],\n",
        "    ax.grid(True)\n",
        "    acc_line, = ax.plot(range(1,epochs+1),stats[tk]['acc'])\n",
        "    val_line, = ax.plot(range(1,epochs+1),stats[tk]['val_acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svLappBwQ_kd"
      },
      "source": [
        "###Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTdOu7kTTneQ"
      },
      "source": [
        "### Collect and Display Evaluation results for all models and all noise levels\n",
        "results = {}\n",
        "for key in training_keys:\n",
        "    model = get_model[key]\n",
        "    print (\">>>>>>>>>>>>>>> Key:\", key)\n",
        "    for nl in noise_levels:\n",
        "        evalkey = (model.name, nl)\n",
        "        x, y = test_data[nl]\n",
        "        evaluated = model.evaluate(x=x, \n",
        "                                   y=y, \n",
        "                                   verbose=0, \n",
        "                                   batch_size=validation_batch_size)\n",
        "        results.update({evalkey:round(evaluated[1], 4)})\n",
        "        condition = describe_nl[nl]\n",
        "        print(model.name,\"tested on\",condition,\":\",round(evaluated[1], 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xztL8dDrYh-G"
      },
      "source": [
        "### Build a DataFrame to hold the results of evaluation \n",
        "rows = len(training_keys)\n",
        "results_df = pd.DataFrame(columns=[\"model\"]+[\"trained_on\"]+noise_levels, index=range(rows))\n",
        "for row, (model_kind, train_condition) in enumerate(training_keys):\n",
        "    results_df.iloc[row][\"model\"]=model_kind\n",
        "    results_df.iloc[row][\"trained_on\"]=train_condition\n",
        "    for test_condition in noise_levels:\n",
        "        results_df.iloc[row][test_condition] = results[(model_kind+\"_\"+train_condition, test_condition)]\n",
        "results_df.sort_values([\"model\",\"trained_on\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RIqnIUPUYWk"
      },
      "source": [
        "## Display baseline results for the no-noise training condition\n",
        "f = plt.figure(figsize=(6,5))\n",
        "condax = f.add_subplot(1,1,1)\n",
        "condax.set_ylim(ymin=0,ymax=1.0),\n",
        "r_df = results_df.loc[lambda df: df[\"trained_on\"]==\"no\"].sort_values('model')\n",
        "r_df.plot(  x=\"model\", \n",
        "            y=[\"no\",\"high\",\"low\",\"var\"], \n",
        "            kind='bar', \n",
        "            ax=condax,            \n",
        "            title=\"Trained on No Noise\", \n",
        "            ylabel=\"Test Accuracy\",\n",
        "            legend=False,\n",
        "            grid=True,\n",
        "            yticks=[0.01]+[t/10 for t in range(2,11,2)]+[0.95],\n",
        "            )\n",
        "ticks = plt.xticks(rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TLOqVU4Lt23"
      },
      "source": [
        "## Plot results for each training condition (noise level) ####\n",
        "f = plt.figure(figsize=(13,12))\n",
        "for i,nl in enumerate(noise_levels): \n",
        "    condax = f.add_subplot(2,2,i+1)\n",
        "    condax.set_ylim(ymin=0,ymax=1.0),\n",
        "    r_df = results_df.loc[lambda df: df[\"trained_on\"]==nl].sort_values('model')\n",
        "    r_df.plot(\"model\", \n",
        "              [\"no\",\"high\",\"low\",\"var\"], \n",
        "              kind='bar', \n",
        "              ax=condax, \n",
        "              title=\"Trained on \"+nl+\" noise\", \n",
        "              ylabel=\"Evaluation Accuracy\",\n",
        "              xlabel=\"\",\n",
        "              legend=False, #i==0,\n",
        "              grid=True,\n",
        "              yticks=[0.01]+[t/10 for t in range(2,11,2)]+[0.95],\n",
        "              sharey=True,\n",
        "              )\n",
        "    plt.xticks(rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjNH8jtlJcxq"
      },
      "source": [
        "## Plot results for baseline model ####\n",
        "f = plt.figure(figsize=(6,6))\n",
        "r_df = results_df.loc[lambda df: df[\"model\"]==\"miniNN\"].sort_values('model')\n",
        "plot = r_df.plot(  x=\"trained_on\", \n",
        "            y=[\"no\",\"high\",\"low\",\"var\"], \n",
        "            kind='bar', \n",
        "            title=\"Baseline Model miniNN\",\n",
        "            xlabel=\"Training Noise\", \n",
        "            ylabel=\"Evaluation Accuracy\",\n",
        "            yticks=[0.01]+[t/10 for t in range(2,11,2)]+[0.95],\n",
        "            sharey=True,\n",
        "            legend=False, #i==0,\n",
        "            grid=True,\n",
        "            )\n",
        "lo, la = plt.xticks(rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyHdIHYbdO11"
      },
      "source": [
        "## Plot results for each model ####\n",
        "f = plt.figure(figsize=(13,12))\n",
        "mks = model_kinds.copy()\n",
        "mks.remove(\"miniNN\")\n",
        "for i, m in enumerate(mks):\n",
        "    r_df = results_df.loc[lambda df: df[\"model\"]==m].sort_values('model')\n",
        "    r_df.plot(x=\"trained_on\", \n",
        "              y=[\"no\",\"high\",\"low\",\"var\"], \n",
        "              kind='bar', \n",
        "              ax=f.add_subplot(2,2,i+1),\n",
        "              title=\"Model \"+m,\n",
        "              xlabel=\"Training Noise\", \n",
        "              ylabel=\"Evaluation Accuracy\",\n",
        "              yticks=[0.01]+[t/10 for t in range(2,10,2)]+[0.95],\n",
        "              sharey=True,\n",
        "              legend=False, #i==0,\n",
        "              grid=True,\n",
        "              )\n",
        "    plt.xticks(rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}