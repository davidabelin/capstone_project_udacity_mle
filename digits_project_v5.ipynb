{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digits_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHK6DyunSbs4"
      },
      "source": [
        "# Machine learning algorithms for recognizing handwritten double-digit images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89yXy-JjuNgy"
      },
      "source": [
        "##Import the modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLy3pthUS0D2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random as rd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        " \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7WCZJZnuEqx"
      },
      "source": [
        "## Set the Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8Y3AFF5pgJZ"
      },
      "source": [
        "### Load raw MNIST data as tuples of numpy arrays\n",
        "### Each tuple is: (examples of images, corresponding labels)\n",
        "traintuple, testuple = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "### Set values for data construction\n",
        "image_size = (28,56) # shape of target image\n",
        "model_kinds = [\"miniNN\", \"CNN\", \"DNN\", \"comboNN\", \"ocNN\"]\n",
        "noise_levels = [\"no\", \"high\", \"low\", \"var\"]\n",
        "training_keys = []\n",
        "for nc in noise_levels:\n",
        "    for k in model_kinds:\n",
        "        training_keys.append((k, nc))\n",
        "\n",
        "### Dict to translate noise level to description\n",
        "noise2condition = {nc:d for (nc, d) in zip(noise_levels,\n",
        "                                        [\"No noise\", \"Low noise\", \"High noise\", \"Variable noise\"])} \n",
        "\n",
        "### Number of images to generate for each category\n",
        "N_train = 100000    # for training\n",
        "N_test = 10000      # for evaluation\n",
        "N_val = 5000        # for use while training\n",
        "\n",
        "### Training hyperparameters (same for all models)\n",
        "batch_size = 500\n",
        "validation_batch_size = 100\n",
        "epochs = 5\n",
        "learning_rate = 0.0025\n",
        "input_layer = layers.Input(shape=(28, 56, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY6KJV6z6l7_"
      },
      "source": [
        "# Construct the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW-Qn7I3QrZQ"
      },
      "source": [
        "### Construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X6jzIZm1vDZ"
      },
      "source": [
        "############ Define Functions for data construction\n",
        "\n",
        "def scale_array(arr):\n",
        "    '''Scale an array of numbers.\n",
        "        Params: numpy array of values to be scaled\n",
        "        Returns: the same array with values scaled to (0,1)\n",
        "    '''\n",
        "    max = np.max(arr)\n",
        "    min = np.min(arr)\n",
        "    scaled = (arr - min)/(max - min)\n",
        "    return scaled\n",
        "print(\"Loaded function scale_array(arr)\")\n",
        "\n",
        "def get_noise(image, noise_condition=\"no\"):\n",
        "    ''' Add a normal disitribution of noise to an image\n",
        "        Params: image: numpy array with values (0,1)\n",
        "                noise_factor: one of \"no\", \"low\", \"high\", \"var\"\n",
        "        Returns: noisy image array re-normalized to (0,1)\n",
        "    '''\n",
        "    cond2num = {\"no\":0, \n",
        "                \"low\":0.3, \n",
        "                \"high\":1.3, \n",
        "                \"var\":1.3*rd.random()} # Different value each call, (0,1.3*std)\n",
        "    noise_factor = np.std(image)*cond2num[noise_condition]\n",
        "    noise = np.random.normal(np.mean(image), \n",
        "                             noise_factor, \n",
        "                             size=image.shape)\n",
        "    # RE-normalize combined image back to (0,1)\n",
        "    return scale_array(noise + image)\n",
        "print(\"Loaded function get_noise(image, noise_condition)\")\n",
        "\n",
        "def doubleDigits(datatuple, nc=\"no\"):\n",
        "    ''' Merge two single-digit images into one double-digit image.\n",
        "        Params: images: np.array with shape (N,28,28), values 0 to 255\n",
        "                answers: np.array with shape (N,), values 0 to 9\n",
        "                nf: \"noise factor\" (float) is multiples/fractions of image std\n",
        "        Returns: noisy double-digit image as a numpy array with shape (28,56),\n",
        "                with values normed to (0.0, 1.0); \n",
        "                and the corresponding label, with values now 0 to 99\n",
        "    '''\n",
        "    (images,answers)=datatuple  # = (x, y)\n",
        "    \n",
        "    # Randomly select left and right single digit images\n",
        "    # with values 0 to 9, from the same raw training or test data set\n",
        "    left_index = rd.randrange(0, len(answers))   \n",
        "    right_index = rd.randrange(0, len(answers))\n",
        "    # Calculate double digit label 0 to 99\n",
        "    answer = answers[left_index]*10 + answers[right_index]\n",
        "    \n",
        "    # Have to scale them here because they have\n",
        "    # different distributions of pixel values\n",
        "    left_scaled = scale_array(images[left_index])\n",
        "    right_scaled = scale_array(images[right_index])\n",
        "\n",
        "    # Make background array with shape (28,56)\n",
        "    image = np.zeros(image_size)\n",
        "    # Group digits closer to middle of new image\n",
        "    width = image_size[1]\n",
        "    half_width = width//2  \n",
        "    # Shift left digit to the right\n",
        "    image[:,8:half_width+4] = left_scaled[:,4:half_width]\n",
        "    # Shift right digit to the left\n",
        "    image[:,half_width-4:width-8] += right_scaled[:,0:half_width-4]\n",
        "    # Have to clip in case some bright pixels overlap\n",
        "    image = image.clip(0,1)\n",
        "    # Add noise to the new double digit image\n",
        "    # get_noise() will overlay a Normal distribution of\n",
        "    # random pixel values centered on the mean of pixel values\n",
        "    # in the image, and with \"noise_factor\" as the width of the\n",
        "    # distribution in multiples or fractions of the \n",
        "    # standard deviation of the pixel values in the image:\n",
        "    # 0 = no distortion, +inf = uniform distribution  \n",
        "    # Rescale pixel values to (0,1) (again!)\n",
        "    image = get_noise(image, nc)  \n",
        "    return image, int(answer)              \n",
        "print(\"Loaded function doubleDigits(datatuple, nc=no)\")\n",
        "\n",
        "def getDoubleDigits(datatuple, how_many=1, nc=\"no\"):\n",
        "    ''' Aggregate a specified number of two-digit images, with or without noise\n",
        "        Params: image array of size (N, (image size)),\n",
        "                answers array of size (N,)\n",
        "        Returns: a single 28x56 double-digit image and\n",
        "                 the corresponding array of int labels\n",
        "    '''\n",
        "    yy = np.zeros((how_many,),dtype=int)\n",
        "    xx = np.zeros((how_many, image_size[0], image_size[1]))\n",
        "    for i in range(how_many):\n",
        "        dd, ans = doubleDigits(datatuple, nc)\n",
        "        yy[i] = ans\n",
        "        xx[i] = dd\n",
        "    return (xx, yy) # (new image, corresponding label)\n",
        "print(\"Loaded function getDoubleDigits(datatuple,how_many=1,nc=no)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YQljE-wizDw"
      },
      "source": [
        "### Generate double-digit training, testing, and validation images\n",
        "### One set for each noise level: no, low, high, and var\n",
        "\n",
        "train_data = {} # {noise level : tuple of training data (x,y)}\n",
        "val_data = {}   # {noise level : val data (x,y)}\n",
        "test_data = {}  # {noise level : test data (x,y)}\n",
        "\n",
        "for nc in noise_levels:\n",
        "    print (\"Noise level:\", noise2condition[nc])\n",
        "    if nc is \"no\":\n",
        "        ### Make a set of training, validation, and test images\n",
        "        ### First set with no noise added as a baseline\n",
        "        # Training set constructed from training set in MNIST\n",
        "        x_train, y_train = getDoubleDigits(traintuple, N_train, nc=nc)\n",
        "        print(\"Made\",N_train,\"new double-digit images to train on.\")\n",
        "        # Test and validation sets constructed from test images in MNIST\n",
        "        x_val, y_val = getDoubleDigits(testuple, N_val, nc=nc)\n",
        "        print(\"Made\",N_val,\"new double-digit images to validate on.\")\n",
        "        x_test, y_test = getDoubleDigits(testuple, N_test, nc=nc)\n",
        "        print(\"Made\",N_test,\"new double-digit images to test on.\")\n",
        "    else:\n",
        "        # Add noise to the baseline \"no noise\" set \n",
        "        x0_train, y_train = train_data[\"no\"]\n",
        "        x0_val, y_val = val_data[\"no\"]\n",
        "        x0_test, y_test = test_data[\"no\"]\n",
        "        x_train = np.array([get_noise(img.reshape(image_size),nc) for img in x0_train])\n",
        "        print(\"Added\",noise2condition[nc],\"to\",N_train,\"double-digit images for training.\")\n",
        "        x_val = np.array([get_noise(img.reshape(image_size),nc) for img in x0_val])\n",
        "        print(\"Added\",noise2condition[nc],\"to\",N_val,\"double-digit images for validation.\")\n",
        "        x_test = np.array([get_noise(img.reshape(image_size),nc) for img in x0_test])\n",
        "        print(\"Added\",noise2condition[nc],\"to\",N_test,\"double-digit images for evaluation.\")        \n",
        "    \n",
        "    ### Add a black-and-white channel dimension\n",
        "    x_train = x_train[..., np.newaxis].astype(\"float32\")\n",
        "    x_val = x_val[..., np.newaxis].astype(\"float32\")\n",
        "    x_test = x_test[..., np.newaxis].astype(\"float32\")\n",
        "\n",
        "    ### Update dict for converting from noiselevel to dataset\n",
        "    train_data.update({nc:(x_train, y_train)})\n",
        "    val_data.update({nc:(x_val, y_val)}) \n",
        "    test_data.update({nc:(x_test, y_test)})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBBGZTy4qFrJ"
      },
      "source": [
        "### Data distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tmD7E3wqlfj"
      },
      "source": [
        "idx = rd.randrange(val_data[\"high\"][0].shape[0])\n",
        "xtd0 = pd.DataFrame(val_data[\"no\"][0].reshape(-1,28*56)).iloc[idx]\n",
        "xtdh = pd.DataFrame(val_data[\"high\"][0].reshape(-1,28*56)).iloc[idx]\n",
        "xtdl = pd.DataFrame(val_data[\"low\"][0].reshape(-1,28*56)).iloc[idx]\n",
        "xtdv = pd.DataFrame(val_data[\"var\"][0].reshape(-1,28*56)).iloc[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz6kG3ypFKcx"
      },
      "source": [
        "#xtd0.hist()\n",
        "#xtdh.hist()\n",
        "#xtdl.hist()\n",
        "xtdv.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZAWz3XoGfDP"
      },
      "source": [
        "plt.imshow(np.array(xtdv).reshape(image_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9dZL6AXZ9rf"
      },
      "source": [
        "arr0_test = np.zeros((x0_test.shape[0],2))\n",
        "for i, img in enumerate(x0_test):\n",
        "    #img.reshape(image_size)\n",
        "    arr0_test[i,0] = img.mean()\n",
        "    arr0_test[i,1] = img.std()\n",
        "\n",
        "arr1_test = np.zeros((test_data[\"low\"][0].shape[0],2))\n",
        "for i, img in enumerate(test_data[\"low\"][0]):\n",
        "    #img.reshape(image_size)\n",
        "    arr1_test[i,0] = img.mean()\n",
        "    arr1_test[i,1] = img.std()\n",
        "\n",
        "arr2_test = np.zeros((test_data[\"high\"][0].shape[0],2))\n",
        "for i, img in enumerate(test_data[\"high\"][0]):\n",
        "    #img.reshape(image_size)\n",
        "    arr2_test[i,0] = img.mean()\n",
        "    arr2_test[i,1] = img.std()\n",
        "\n",
        "arr3_test = np.zeros((test_data[\"var\"][0].shape[0],2))\n",
        "for i, img in enumerate(test_data[\"var\"][0]):\n",
        "    #img.reshape(image_size)\n",
        "    arr3_test[i,0] = img.mean()\n",
        "    arr3_test[i,1] = img.std()\n",
        "\n",
        "\n",
        "print(arr0_test.mean(), arr0_test.std())\n",
        "print(arr1_test.mean(), arr1_test.std())\n",
        "print(arr2_test.mean(), arr2_test.std())\n",
        "print(arr3_test.mean(), arr3_test.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2rxp_Hoqd1p"
      },
      "source": [
        "### Sample set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiFcn-Bm4dnX"
      },
      "source": [
        "### Check a random sample of images for each noise level\n",
        "\n",
        "x0, y0 = getDoubleDigits(traintuple, nc=\"no\", how_many=5)\n",
        "print('Answers:',y0) \n",
        "\n",
        "x1 = np.array([get_noise(img.reshape(image_size),\"low\") for img in x0])\n",
        "x2 = np.array([get_noise(img.reshape(image_size),\"high\") for img in x0])\n",
        "x3 = np.array([get_noise(img.reshape(image_size),\"var\") for img in x0])\n",
        "\n",
        "plt.set_cmap('viridis')\n",
        "f,a = plt.subplots(4,5,True,True,)\n",
        "f.set_size_inches(18,8)\n",
        "a=a.reshape(20,)\n",
        "for i, k in enumerate([x0,x1,x2,x3]):\n",
        "    for j in range(5): \n",
        "        a[5*i+j].imshow(k[j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQCcBks_ic6L"
      },
      "source": [
        "plt.set_cmap('binary')\n",
        "f,a = plt.subplots(4,5,True,True)\n",
        "f.set_size_inches(18,8)\n",
        "a=a.reshape(20,)\n",
        "for i in range(5): \n",
        "    a[i].imshow(x0[i])\n",
        "    a[i+5].imshow(x1[i])\n",
        "    a[i+10].imshow(x2[i])\n",
        "    a[i+15].imshow(x3[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oqBkNBJmtUv"
      },
      "source": [
        "# Build and Train the Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtngjIXgUvws"
      },
      "source": [
        "### Define functions for building, training, compiling and evaluating models\n",
        "\n",
        "def build_model(key, verbose=True):\n",
        "    '''Builds a model of each kind, for each noise level\n",
        "        params: a tuple of \"model kind\" and \"noise level\"\n",
        "                verbose turns on printed reports\n",
        "        returns: a model of the specified kind, to be\n",
        "                 dedicated to noise of the specified level\n",
        "    '''\n",
        "    (model_kind, noise) = key\n",
        "    ## Input layer, same for for all models\n",
        "    input_layer = layers.Input(shape=(image_size[0], image_size[1], 1))\n",
        "        \n",
        "    if \"miniNN\" in model_kind:\n",
        "        ########### Build model miniNN as a minimal, baseline model \n",
        "        x = layers.Flatten()(input_layer)#\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        miniNN = Model(input_layer, output_layer, name=\"miniNN_\"+noise)\n",
        "        if verbose: print (\"built\", miniNN.name)\n",
        "        return miniNN\n",
        "\n",
        "    elif \"CNN\" in model_kind:\n",
        "        ########### Build model CNN: typical convolutional neural network\n",
        "        x = layers.Conv2D(20, 3, activation='relu')(input_layer)#, padding='same'\n",
        "        x = layers.MaxPooling2D(2)(x)\n",
        "        x = layers.Conv2D(30, 2, activation='relu')(x) \n",
        "        x = layers.AveragePooling2D(2)(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        CNN = Model(input_layer, output_layer, name=\"CNN_\"+noise)\n",
        "        if verbose: print (\"built\", CNN.name)\n",
        "        return CNN\n",
        "    \n",
        "    elif \"DNN\" in model_kind:\n",
        "        ########### Build model DNN, a densely connected NN with 4 layers\n",
        "        x = layers.Flatten()(input_layer)\n",
        "        x = layers.Dropout(0.1)(x)\n",
        "        x = layers.Dense(999, activation='relu')(x)\n",
        "        x = layers.Dropout(0.1)(x)\n",
        "        x = layers.Dense(333, activation='relu')(x)\n",
        "        #x = layers.Dropout(0.1)(x) \n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        DNN = Model(input_layer, output_layer, name=\"DNN_\"+noise)\n",
        "        if verbose: print (\"built\", DNN.name)\n",
        "        return DNN\n",
        "\n",
        "    elif \"comboNN\" in model_kind:\n",
        "        ########### Build model comboNN, a hybrid of both convolutional and dense layers \n",
        "        x = layers.Conv2D(20, 3, activation='relu')(input_layer)\n",
        "        x = layers.AveragePooling2D(2)(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dense(720, activation='relu')(x)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        output_layer = layers.Dense(100, activation='softmax')(x)\n",
        "        comboNN = Model(input_layer, output_layer, name=\"comboNN_\"+noise)\n",
        "        if verbose: print (\"built\", comboNN.name)\n",
        "        return comboNN\n",
        "\n",
        "    elif \"ocNN\" in model_kind:\n",
        "        ########### Build \"overcomplicated\" model ocNN, of unnecessary complexity \n",
        "        x = layers.Flatten()(input_layer)\n",
        "        x = layers.Dense(800, activation='relu')(x)\n",
        "        x = layers.Dropout(0.1)(x)\n",
        "        x1 = layers.Dense(600, activation='relu')(x) \n",
        "        x1 = layers.Dropout(0.2)(x1)\n",
        "        x2 = layers.Dense(200, activation='relu')(x) \n",
        "        x2 = layers.Dropout(0.3)(x2)\n",
        "\n",
        "        y = layers.Conv2D(20, 3, activation='relu')(input_layer)\n",
        "        y = layers.MaxPooling2D(2)(y)\n",
        "        y1 = layers.Conv2D(30, 2, padding='same', activation='relu')(y) \n",
        "        y1 = layers.AveragePooling2D(2)(y1)\n",
        "        y1 = layers.Flatten()(y1)\n",
        "        y1 = layers.Dropout(0.2)(y1)\n",
        "        y2 = layers.Conv2D(33, 3, activation='relu')(y) \n",
        "        y2 = layers.Conv2D(22, 2, padding='same', activation='relu')(y2)\n",
        "        y2 = layers.MaxPooling2D(2)(y2)\n",
        "        y2 = layers.Dropout(0.1)(y2)\n",
        "        y2 = layers.Flatten()(y2)\n",
        "        \n",
        "\n",
        "        z1 = layers.Concatenate()([x1,y2])\n",
        "        z1 = layers.Dropout(0.3)(z1)\n",
        "        z2 = layers.Concatenate()([x2,y1])\n",
        "        z2 = layers.Dropout(0.05)(z2)\n",
        "\n",
        "        z = layers.Concatenate()([z1,z2,x])\n",
        "        z = layers.Dense(1111,activation=\"relu\")(z)\n",
        "        z = layers.Dropout(0.2)(z)\n",
        "\n",
        "        logits_layer = layers.Dense(100, activation='softmax')(z) \n",
        "        ocNN = Model(input_layer, logits_layer, name=\"ocNN_\"+noise)\n",
        "        if verbose: print (\"built\", ocNN.name)\n",
        "        return ocNN\n",
        "\n",
        "    else:\n",
        "        print (\"Could not find that kind of model.\")\n",
        "        return None\n",
        "print(\"loaded function build_model((model_kind, noise_level))\")\n",
        "\n",
        "## Same training loop for all models\n",
        "def train(model, traintuple, valtuple, epochs=epochs):\n",
        "    '''Train a model on the given sets of data\n",
        "        Params: the given model,\n",
        "                the train data as a tuple of x,y,\n",
        "                the test data as a tuple of x,y\n",
        "        Returns: a dictionary of metric values after each epoch of training\n",
        "    '''\n",
        "    (x_train, y_train) = traintuple\n",
        "    (x_val, y_val) = valtuple\n",
        "\n",
        "    history = model.fit(x=x_train,\n",
        "                        y=y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=valtuple,\n",
        "                        validation_batch_size=validation_batch_size,\n",
        "                        epochs=epochs,  \n",
        "                        verbose=1)   \n",
        "    return history.history\n",
        "print(\"loaded function train(model, traintuple, testuple, epochs=epochs)\")\n",
        "\n",
        "## All models are compiled the same\n",
        "def compile_model(model):    \n",
        "    model.compile(  loss=\"sparse_categorical_crossentropy\",\n",
        "                    optimizer=RMSprop(lr=learning_rate),                    \n",
        "                    metrics=['acc'])\n",
        "    print (\"Compiled model\", model.name)\n",
        "print(\"loaded function compile_model(model)\")\n",
        "\n",
        "## Plot the learning curves for each model:\n",
        "## accuracy and validation_accuracy after each training epoch\n",
        "def plotLearningCurves(history):\n",
        "    acc = history['acc']\n",
        "    val_acc = history['val_acc']\n",
        "    epochs = range(len(acc))\n",
        "    plt.plot(epochs, acc)\n",
        "    plt.plot(epochs, val_acc)\n",
        "    plt.title('Accuracy after each training epoch')\n",
        "print(\"Loaded function plotLearningCurves(history)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8FZdVmQ_3DP"
      },
      "source": [
        "\n",
        "#get_model.update( {(\"miniNN\",\"no\"):build_model((\"miniNN\",\"no\"),True)} )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP2VqeELLpGX"
      },
      "source": [
        "get_model = {}  # Training key --> model\n",
        "for key in training_keys:  # One training key for each model on each noise level.\n",
        "     model = build_model(key, verbose=False)  \n",
        "     get_model.update({key:model}) # One model for each training key\n",
        "\n",
        "### Compile and Train all 20 models\n",
        "stats = {}\n",
        "for key in training_keys:    \n",
        "    nc = key[1]\n",
        "    model = get_model[key]\n",
        "    compile_model(model)\n",
        "    train_stats = train(model, train_data[nc], val_data[nc], epochs=epochs)\n",
        "    stats.update({key:train_stats})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6XfSpmr6X2n"
      },
      "source": [
        "######### Print out summary tables for all the models\n",
        "for m in model_kinds:\n",
        "    print(get_model[(m,\"no\")].summary(),\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m94196ctr4T"
      },
      "source": [
        "#Evaluate the Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWlI_VvyRE67"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT8BvV8N_6Lv"
      },
      "source": [
        "### Plot the learning curves for all 20 models during training\n",
        "ncols = 5\n",
        "nrows = 4\n",
        "fig = plt.figure(figsize=(16,16)) \n",
        "axarr = fig.subplots(nrows,ncols,sharex=True,sharey=True,)\n",
        "axarr = axarr.reshape((20,))\n",
        "for i, tk in enumerate(training_keys):\n",
        "    ax = axarr[i]\n",
        "    ax.set_title(tk)\n",
        "    ax.set_xlim(0,4)\n",
        "    ax.set_ylim(0,1)\n",
        "    ax.plot(range(epochs),stats[tk]['acc'])\n",
        "    ax.plot(range(epochs),stats[tk]['val_acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svLappBwQ_kd"
      },
      "source": [
        "###Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTdOu7kTTneQ"
      },
      "source": [
        "### Collect Evaluation results for all models and all noise levels\n",
        "\n",
        "results = {}\n",
        "evalkeys = []\n",
        "for key in training_keys:\n",
        "    model = get_model[key]\n",
        "    print (\">>>>>>>>>>>>>>> Key:\", key)\n",
        "    for nl in noise_levels:\n",
        "        evalkey = (model.name, nl)\n",
        "        evalkeys += [evalkey]\n",
        "        x, y = test_data[nl]\n",
        "        evaluated = model.evaluate(x=x, \n",
        "                                   y=y, \n",
        "                                   verbose=0, \n",
        "                                   batch_size=validation_batch_size)\n",
        "        results.update({evalkey:round(evaluated[1], 4)})\n",
        "        condition = noise2condition[nl]\n",
        "        print(model.name,\"tested on\",condition,\":\",round(evaluated[1], 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xztL8dDrYh-G"
      },
      "source": [
        "### Build a DataFrame to hold the results of evaluation \n",
        "rows = len(training_keys)\n",
        "results_df = pd.DataFrame(columns=[\"model\"]+[\"trained_on\"]+noise_levels, index=range(rows))\n",
        "for row, (model_kind, train_condition) in enumerate(training_keys):\n",
        "    results_df.iloc[row][\"model\"]=model_kind\n",
        "    results_df.iloc[row][\"trained_on\"]=train_condition\n",
        "    for test_condition in noise_levels:\n",
        "        results_df.iloc[row][test_condition] = results[(model_kind+\"_\"+train_condition, test_condition)]\n",
        "results_df = results_df.sort_values([\"model\",\"trained_on\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RIqnIUPUYWk"
      },
      "source": [
        "## Graph of baseline results\n",
        "## Plot results for each training condition (noise level) ####\n",
        "f = plt.figure(figsize=(7,4))\n",
        "for i,nl in enumerate([\"no\"]):\n",
        "    #leg = i==0\n",
        "    condax = f.add_subplot(1,1,i+1)\n",
        "    r_df = results_df.loc[lambda df: df[\"trained_on\"]==nl].sort_values('model')\n",
        "    r_df.plot(\"model\", \n",
        "              [\"no\"], \n",
        "              kind='bar', \n",
        "              ax=condax, \n",
        "              title=\"Baseline: no noise for training, no noise for evaluation\", \n",
        "              ylabel=\"Evaluation Accuracy\",\n",
        "              xlabel=\"Baseline Models\",\n",
        "              legend=False,\n",
        "              grid=True,\n",
        "              yticks=[0.01]+[t/10 for t in range(1,10,2)],\n",
        "              )\n",
        "    plt.xticks(rotation=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyHdIHYbdO11"
      },
      "source": [
        "## Plot results for each model ####\n",
        "f = plt.figure(figsize=(15,17))\n",
        "\n",
        "for i, m in enumerate(model_kinds):\n",
        "    modax = f.add_subplot(3,2,i+1)\n",
        "    modax.set(ylim=(0, 1))\n",
        "    modax.sharey=True\n",
        "    r_df = results_df.loc[lambda df: df[\"model\"]==m].sort_values('model')\n",
        "    r_df.plot(\"trained_on\", \n",
        "              [\"no\",\"high\",\"low\",\"var\"], \n",
        "              kind='bar', \n",
        "              ax=modax,\n",
        "              title=\"Model \"+m, \n",
        "              ylabel=\"Evaluation Accuracy\",\n",
        "              xlabel=\"Training Noise\",\n",
        "              legend=i==0,\n",
        "              grid=True,\n",
        "              yticks=[0.01]+[t/10 for t in range(1,10,2)],\n",
        "              #sharey=True\n",
        "              )\n",
        "    plt.xticks(rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TLOqVU4Lt23"
      },
      "source": [
        "## Plot results for each training condition (noise level) ####\n",
        "f = plt.figure(figsize=(15,12))\n",
        "for i,nl in enumerate(noise_levels):\n",
        "    leg = i==0\n",
        "    condax = f.add_subplot(2,2,i+1)\n",
        "    r_df = results_df.loc[lambda df: df[\"trained_on\"]==nl].sort_values('model')\n",
        "    r_df.plot(\"model\", \n",
        "              [\"no\",\"high\",\"low\",\"var\"], \n",
        "              kind='bar', \n",
        "              ax=condax, \n",
        "              title=\"Trained on \"+nl+\" noise\", \n",
        "              ylabel=\"Evaluation Accuracy\",\n",
        "              xlabel=\"\",\n",
        "              legend=leg,\n",
        "              grid=True,\n",
        "              yticks=[0.01]+[t/10 for t in range(1,10,2)],\n",
        "              )\n",
        "    plt.xticks(rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfmFIWmzf74l"
      },
      "source": [
        "crit=results_df[\"trained_on\"]==\"no\"\n",
        "print(results_df.loc[crit])#[\"no\"].drop(\"no\",axis=1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}